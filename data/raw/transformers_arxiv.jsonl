{"id": "2507.18634v1", "title": "Captain Cinema: Towards Short Movie Generation", "authors": ["Junfei Xiao", "Ceyuan Yang", "Lvmin Zhang", "Shengqu Cai", "Yang Zhao", "Yuwei Guo", "Gordon Wetzstein", "Maneesh Agrawala", "Alan Yuille", "Lu Jiang"], "categories": ["cs.CV"], "abstract": "We present Captain Cinema, a generation framework for short movie generation. Given a detailed textual description of a movie storyline, our approach firstly generates a sequence of keyframes that outline the entire narrative, which ensures long-range coherence in both the storyline and visual appearance (e.g., scenes and characters). We refer to this step as top-down keyframe planning. These keyframes then serve as conditioning signals for a video synthesis model, which supports long context learning, to produce the spatio-temporal dynamics between them. This step is referred to as bottom-up video synthesis. To support stable and efficient generation of multi-scene long narrative cinematic works, we introduce an interleaved training strategy for Multimodal Diffusion Transformers (MM-DiT), specifically adapted for long-context video data. Our model is trained on a specially curated cinematic dataset consisting of interleaved data pairs. Our experiments demonstrate that Captain Cinema performs favorably in the automated creation of visually coherent and narrative consistent short movies in high quality and efficiency. Project page: https://thecinema.ai", "published": "2025-07-24T17:59:56+00:00"}
{"id": "2507.18625v1", "title": "3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation", "authors": ["Shuqing Li", "Anson Y. Lam", "Yun Peng", "Wenxuan Wang", "Michael R. Lyu"], "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SE"], "abstract": "Graphical user interface (UI) software has undergone a fundamental transformation from traditional two-dimensional (2D) desktop/web/mobile interfaces to spatial three-dimensional (3D) environments. While existing work has made remarkable success in automated 2D software generation, such as HTML/CSS and mobile app interface code synthesis, the generation of 3D software still remains under-explored. Current methods for 3D software generation usually generate the 3D environments as a whole and cannot modify or control specific elements in the software. Furthermore, these methods struggle to handle the complex spatial and semantic constraints inherent in the real world. To address the challenges, we present Scenethesis, a novel requirement-sensitive 3D software synthesis approach that maintains formal traceability between user specifications and generated 3D software. Scenethesis is built upon ScenethesisLang, a domain-specific language that serves as a granular constraint-aware intermediate representation (IR) to bridge natural language requirements and executable 3D software. It serves both as a comprehensive scene description language enabling fine-grained modification of 3D software elements and as a formal constraint-expressive specification language capable of expressing complex spatial constraints. By decomposing 3D software synthesis into stages operating on ScenethesisLang, Scenethesis enables independent verification, targeted modification, and systematic constraint satisfaction. Our evaluation demonstrates that Scenethesis accurately captures over 80% of user requirements and satisfies more than 90% of hard constraints while handling over 100 constraints simultaneously. Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual evaluation scores compared to the state-of-the-art method.", "published": "2025-07-24T17:58:03+00:00"}
{"id": "2507.18620v1", "title": "Strong CP Phase and Parity in the Hamiltonian Formalism", "authors": ["Ravi Kuchimanchi"], "categories": ["hep-ph", "hep-ex", "hep-th"], "abstract": "Solutions to the strong CP problem based on P or CP symmetries are typically framed using the Lagrangian formalism. In this work, we analyze the strong CP phase in QCD from the Hamiltonian perspective, focusing on the invariance of the Hamiltonian H under P (or a generalized parity operator $\\mathcal{P}$). For $\\mathcal{P}$ to be a physical symmetry, it must preserve the Hilbert space $\\mathcal{H}_\\theta$ associated with the $\\theta$-vacuum (i.e., $\\mathcal{P}: \\mathcal{H}_\\theta \\rightarrow \\mathcal{H}_\\theta$). This requirement implies that the strong CP phase $\\bar{\\theta} = \\theta + \\arg \\det M$ must vanish, i.e., $\\theta$ must cancel the phase of the quark mass matrix $M$. Equivalently, we show that this condition follows from requiring that parity commute with the large gauge transformation operator of QCD.", "published": "2025-07-24T17:55:44+00:00"}
{"id": "2507.18615v1", "title": "Rapidity-Dependent Spin Decomposition of the Nucleon", "authors": ["Florian Hechenberger", "Kiminad A. Mamo", "Ismail Zahed"], "categories": ["hep-ph", "nucl-th"], "abstract": "We show that the two-dimensional Fourier transform of the generalized parton distributions (GPDs) has two distinct interpretations: at zero skewness ($\\eta=0$) it yields the familiar impact-parameter density, while at finite skewness ($\\eta \\neq 0$) it encodes a genuine parton-nucleon correlation whose norm decreases predictably with the rapidity gap $\\Delta y = 2\\mathrm{artanh} \\eta$. This rapidity dependence produces universal rapidity-modified Ji identities linking helicity, orbital, and total angular momenta analytically. Using linear open- and closed string Regge trajectories constrained by empirical PDFs, spectroscopy and form factor data, we obtain the leading twist GPDs $H,E,\\widetilde{H}$ across the full $(x,\\eta,t)$ range. Numerical Mellin Barnes inversion agrees with existing lattice data and yields rapidity resolved predictions for Jefferson Lab 12 GeV, the Electron Ion Collider, and forthcoming lattice studies.", "published": "2025-07-24T17:50:42+00:00"}
{"id": "2507.18614v1", "title": "Galaxy quenching across the Cosmic Web: disentangling mass and environment with SDSS DR18", "authors": ["Anindita Nandi", "Biswajit Pandey"], "categories": ["astro-ph.GA", "astro-ph.CO"], "abstract": "We investigate the influence of large-scale cosmic web environments on galaxy quenching using a volume-limited, stellar mass-matched galaxy sample from SDSS DR18. Galaxies are classified as residing in sheets, filaments, or clusters based on the eigenvalues of the tidal tensor derived from the smoothed density field. The quenched fraction increases with stellar mass and is highest in clusters, intermediate in filaments, and lowest in sheets, reflecting the increasing efficiency of environmental quenching with density. A flattening of the quenched fraction beyond $\\log_{10}(M_\\star/M_\\odot) \\sim 10.6$ across all environments signals a transition from environment-driven to mass-driven quenching. In contrast, the bulge fraction continues to rise beyond this threshold, indicating a decoupling between star formation suppression and morphological transformation. At the high-mass end ($\\log_{10}(M_\\star/M_\\odot) \\gtrsim 11.5$), both quenched and bulge fractions bifurcate, increasing in clusters but declining in sheets, suggesting a divergent evolutionary pathway where massive galaxies in sheets retain cold gas and disk-like morphologies, potentially sustaining or rejuvenating star formation. The AGN fraction also increases with stellar mass and is somewhat higher in sheets than in clusters, indicating enhanced AGN activity in low-density, gas-rich environments. The high-mass trends are independently corroborated by our analysis of specific star formation rate, $(u-r)$ colour, concentration index, and D4000 in the stellar mass-density plane, which show that massive galaxies in sheets remain bluer, younger, more star-forming, and structurally less evolved than their cluster counterparts. Our results highlight the cosmic web as an active driver of galaxy evolution.", "published": "2025-07-24T17:50:20+00:00"}
{"id": "2507.18604v1", "title": "A Microlocal Theory for Zariski-Constructible Sheaves on Rigid Analytic Varieties", "authors": ["Tong Zhou"], "categories": ["math.AG"], "abstract": "We develop a microlocal theory, in the sense of Kashiwara-Schapira, for Zariski-constructible sheaves on rigid analytic varieties. We define and study monodromic sheaves, the monodromic Fourier transform, specialisation, microlocalisation, micro-hom, and singular support in this context. Some questions and conjectures are formulated in the end. The appendix contains infinity-categorical characterisations of monodromic sheaves.", "published": "2025-07-24T17:37:38+00:00"}
{"id": "2507.18597v1", "title": "Linear Memory SE(2) Invariant Attention", "authors": ["Ethan Pronovost", "Neha Boloor", "Peter Schleede", "Noureldin Hendy", "Andres Morales", "Nicholas Roy"], "categories": ["cs.LG"], "abstract": "Processing spatial data is a key component in many learning tasks for autonomous driving such as motion forecasting, multi-agent simulation, and planning. Prior works have demonstrated the value in using SE(2) invariant network architectures that consider only the relative poses between objects (e.g. other agents, scene features such as traffic lanes). However, these methods compute the relative poses for all pairs of objects explicitly, requiring quadratic memory. In this work, we propose a mechanism for SE(2) invariant scaled dot-product attention that requires linear memory relative to the number of objects in the scene. Our SE(2) invariant transformer architecture enjoys the same scaling properties that have benefited large language models in recent years. We demonstrate experimentally that our approach is practical to implement and improves performance compared to comparable non-invariant architectures.", "published": "2025-07-24T17:28:57+00:00"}
{"id": "2507.18591v1", "title": "An omnibus goodness-of-fit test based on trigonometric moments", "authors": ["Alain Desgagn\u00e9", "Fr\u00e9d\u00e9ric Ouimet"], "categories": ["stat.ME", "math.ST", "stat.AP", "stat.TH", "62F03, 60F05, 62E20, 62F12, 62H10, 62H12, 62H15"], "abstract": "We present a versatile omnibus goodness-of-fit test based on the first two trigonometric moments of probability-integral-transformed data, which rectifies the covariance scaling errors made by Langholz and Kronmal [J. Amer. Statist. Assoc. 86 (1991), 1077--1084]. Once properly scaled, the quadratic-form statistic asymptotically follows a $\\chi_2^2$ distribution under the null hypothesis. The covariance scalings and parameter estimators are provided for $32$ null distribution families, covering heavy-tailed, light-tailed, asymmetric, and bounded-support cases, so the test is ready to be applied directly. Using recent advances in non-degenerate multivariate $U$-statistics with estimated nuisance parameters, we also showcase its asymptotic distribution under local alternatives for three specific examples. Our procedure shows excellent power; in particular, simulations testing the Laplace model against a range of $400$ alternatives reveal that it surpasses all $40$ existing tests for moderate to large sample sizes. A real-data application involving 48-hour-ahead surface temperature forecast errors further demonstrates the practical utility of the test. To ensure full reproducibility, the R code that generated our numerical results is publicly accessible online.", "published": "2025-07-24T17:17:16+00:00"}
{"id": "2507.18587v1", "title": "A Foundation Model for Massive MIMO Precoding with an Adaptive per-User Rate-Power Tradeoff", "authors": ["J\u00e9r\u00f4me Emery", "Ali Hasanzadeh Karkan", "Jean-Fran\u00e7ois Frigon", "Fran\u00e7ois Leduc-Primeau"], "categories": ["eess.SP", "cs.AI"], "abstract": "Deep learning (DL) has emerged as a solution for precoding in massive multiple-input multiple-output (mMIMO) systems due to its capacity to learn the characteristics of the propagation environment. However, training such a model requires high-quality, local datasets at the deployment site, which are often difficult to collect. We propose a transformer-based foundation model for mMIMO precoding that seeks to minimize the energy consumption of the transmitter while dynamically adapting to per-user rate requirements. At equal energy consumption, zero-shot deployment of the proposed foundation model significantly outperforms zero forcing, and approaches weighted minimum mean squared error performance with 8x less complexity. To address model adaptation in data-scarce settings, we introduce a data augmentation method that finds training samples similar to the target distribution by computing the cosine similarity between the outputs of the pre-trained feature extractor. Our work enables the implementation of DL-based solutions in practice by addressing challenges of data availability and training complexity. Moreover, the ability to dynamically configure per-user rate requirements can be leveraged by higher level resource allocation and scheduling algorithms for greater control over energy efficiency, spectral efficiency and fairness.", "published": "2025-07-24T17:10:06+00:00"}
{"id": "2507.18586v1", "title": "Implementation of the inverse scattering transform method for the nonlinear Schr\u00f6dinger equation", "authors": ["Vladislav V. Kravchenko"], "categories": ["math.AP", "math-ph", "math.CA", "math.MP", "nlin.SI"], "abstract": "We study the initial-value problem for the nonlinear Schr\\\"odinger equation. Application of the inverse scattering transform method involves solving direct and inverse scattering problems for the Zakharov-Shabat system with complex potentials. We solve these problems by using new series representations for the Jost solutions of the Zakharov-Shabat system. The representations have the form of power series with respect to a transformed spectral parameter. In terms of the representations, solution of the direct scattering problem reduces to computing the series coefficients following a simple recurrent integration procedure, computation of the scattering coefficients by multiplying corresponding pairs of polynomials (partial sums of the series representations) and locating zeros of a polynomial inside the unit disk. Solution of the inverse scattering problem reduces to the solution of a system of linear algebraic equations for the power series coefficients, while the potential is recovered from the first coefficients. The system is obtained directly from the scattering relations. Thus, unlike other existing techniques, the method does not involve solving the Gelfand-Levitan-Marchenko equation or the matrix Riemann-Hilbert problem. The overall approach leads to a simple and efficient algorithm for the numerical solution of the initial-value problem for the nonlinear Schr\\\"odinger equation, which is illustrated by numerical examples.", "published": "2025-07-24T17:09:46+00:00"}
{"id": "2507.18575v1", "title": "HybridTM: Combining Transformer and Mamba for 3D Semantic Segmentation", "authors": ["Xinyu Wang", "Jinghua Hou", "Zhe Liu", "Yingying Zhu"], "categories": ["cs.CV"], "abstract": "Transformer-based methods have demonstrated remarkable capabilities in 3D semantic segmentation through their powerful attention mechanisms, but the quadratic complexity limits their modeling of long-range dependencies in large-scale point clouds. While recent Mamba-based approaches offer efficient processing with linear complexity, they struggle with feature representation when extracting 3D features. However, effectively combining these complementary strengths remains an open challenge in this field. In this paper, we propose HybridTM, the first hybrid architecture that integrates Transformer and Mamba for 3D semantic segmentation. In addition, we propose the Inner Layer Hybrid Strategy, which combines attention and Mamba at a finer granularity, enabling simultaneous capture of long-range dependencies and fine-grained local features. Extensive experiments demonstrate the effectiveness and generalization of our HybridTM on diverse indoor and outdoor datasets. Furthermore, our HybridTM achieves state-of-the-art performance on ScanNet, ScanNet200, and nuScenes benchmarks. The code will be made available at https://github.com/deepinact/HybridTM.", "published": "2025-07-24T16:48:50+00:00"}
{"id": "2507.18555v1", "title": "Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU Networks with Random Hidden Weights", "authors": ["Jun'ichi Takeuchia", "Yoshinari Takeishia", "Noboru Muratab", "Kazushi Mimurac", "Ka Long Keith Hod", "Hiroshi Nagaoka"], "categories": ["cs.LG", "stat.ML"], "abstract": "Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU networks with random hidden weight are argued. We discuss the relation between both notions as a linear transformation and show that spectral decomposition of NTK with concrete forms of eigenfunctions with major eigenvalues. We also obtain an approximation formula of the functions presented by the 2-layer neural networks.", "published": "2025-07-24T16:26:52+00:00"}
{"id": "2507.18553v1", "title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm", "authors": ["Jiale Chen", "Torsten Hoefler", "Dan Alistarh"], "categories": ["cs.LG"], "abstract": "Quantizing the weights of large language models (LLMs) from 16-bit to lower bitwidth is the de facto approach to deploy massive transformers onto more affordable accelerators. GPTQ emerged as one of the standard methods for one-shot post-training quantization at LLM scale. Yet, its inner workings are described as a sequence of ad-hoc algebraic updates that obscure any geometric meaning or worst-case guarantees. In this work, we show that, when executed back-to-front (from the last to first dimension) for a linear layer, GPTQ is mathematically identical to Babai's nearest plane algorithm for the classical closest vector problem (CVP) on a lattice defined by the Hessian matrix of the layer's inputs. This equivalence is based on a sophisticated mathematical argument, and has two analytical consequences: (i) the GPTQ error propagation step gains an intuitive geometric interpretation; (ii) GPTQ inherits the error upper bound of Babai's algorithm under the no-clipping condition. Taken together, these results place GPTQ on firm theoretical footing and open the door to importing decades of progress in lattice algorithms towards the design of future quantization algorithms for billion-parameter models.", "published": "2025-07-24T16:22:18+00:00"}
{"id": "2507.18546v1", "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface", "authors": ["Urchade Zaratiana", "Gil Pasternak", "Oliver Boyd", "George Hurn-Maloney", "Ash Lewis"], "categories": ["cs.CL", "cs.AI"], "abstract": "Information extraction (IE) is fundamental to numerous NLP applications, yet existing solutions often require specialized models for different tasks or rely on computationally expensive large language models. We present GLiNER2, a unified framework that enhances the original GLiNER architecture to support named entity recognition, text classification, and hierarchical structured data extraction within a single efficient model. Built pretrained transformer encoder architecture, GLiNER2 maintains CPU efficiency and compact size while introducing multi-task composition through an intuitive schema-based interface. Our experiments demonstrate competitive performance across extraction and classification tasks with substantial improvements in deployment accessibility compared to LLM-based alternatives. We release GLiNER2 as an open-source pip-installable library with pre-trained models and documentation at https://github.com/fastino-ai/GLiNER2.", "published": "2025-07-24T16:11:14+00:00"}
{"id": "2507.18540v1", "title": "Deep Variational Free Energy Calculation of Hydrogen Hugoniot", "authors": ["Zihang Li", "Hao Xie", "Xinyang Dong", "Lei Wang"], "categories": ["cond-mat.str-el", "cs.LG", "physics.comp-ph"], "abstract": "We develop a deep variational free energy framework to compute the equation of state of hydrogen in the warm dense matter region. This method parameterizes the variational density matrix of hydrogen nuclei and electrons at finite temperature using three deep generative models: a normalizing flow model that represents the Boltzmann distribution of the classical nuclei, an autoregressive transformer that models the distribution of electrons in excited states, and a permutational equivariant flow model that constructs backflow coordinates for electrons in Hartree-Fock orbitals. By jointly optimizing the three neural networks to minimize the variational free energy, we obtain the equation of state and related thermodynamic properties of dense hydrogen. We compare our results with other theoretical and experimental results on the deuterium Hugoniot curve, aiming to resolve existing discrepancies. The calculated results provide a valuable benchmark for deuterium in the warm dense matter region.", "published": "2025-07-24T16:07:13+00:00"}
{"id": "2507.18534v1", "title": "Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models", "authors": ["Xingyu Qiu", "Mengying Yang", "Xinghua Ma", "Dong Liang", "Yuzhen Li", "Fanding Li", "Gongning Luo", "Wei Wang", "Kuanquan Wang", "Shuo Li"], "categories": ["cs.CV", "cs.LG"], "abstract": "EDM elucidates the unified design space of diffusion models, yet its fixed noise patterns restricted to pure Gaussian noise, limit advancements in image restoration. Our study indicates that forcibly injecting Gaussian noise corrupts the degraded images, overextends the image transformation distance, and increases restoration complexity. To address this problem, our proposed EDA Elucidates the Design space of Arbitrary-noise-based diffusion models. Theoretically, EDA expands the freedom of noise pattern while preserving the original module flexibility of EDM, with rigorous proof that increased noise complexity incurs no additional computational overhead during restoration. EDA is validated on three typical tasks: MRI bias field correction (global smooth noise), CT metal artifact reduction (global sharp noise), and natural image shadow removal (local boundary-aware noise). With only 5 sampling steps, EDA outperforms most task-specific methods and achieves state-of-the-art performance in bias field correction and shadow removal.", "published": "2025-07-24T16:01:34+00:00"}
{"id": "2507.18530v1", "title": "Deep learning-enabled large-scale analysis of particle geometry-lithiation correlations in battery cathode materials", "authors": ["Binbin Lin", "Luis J. Carrillo", "Xiang-Long Peng", "Wan-Xin Chen", "David A. Santosb", "Sarbajit Banerjeeb", "Bai-Xiang Xu"], "categories": ["cond-mat.mtrl-sci"], "abstract": "A deep learning model is employed to address the challenging problem of V2O5 nanoparticle segmentation and the correlation between the chemical composition and the geometrical features of lithiated V2O5 nanoparticles as an exemplar of a phase-transforming battery cathode material. First, the deep learning-enabled segmentation model is integrated with the singular value decomposition technique and a spectral database to generate accurate composition and phase maps capturing lithiation heterogeneities as imaged using scanning transmission X-ray microscopy. These phase maps act as the output properties for correlation analysis. Subsequently, the quantitative influences of the geometrical features of nanoparticles such as the particle size (i.e., projected perimeter and area), the aspect ratio, circularity, convexity, and orientation on the lithiation phase maps are revealed. These findings inform strategies to improve lithiation uniformity and reduce stress in phase-transforming lithium battery materials via optimized particle geometry.", "published": "2025-07-24T15:58:27+00:00"}
{"id": "2507.18523v1", "title": "The Moral Gap of Large Language Models", "authors": ["Maciej Skorski", "Alina Landowska"], "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG"], "abstract": "Moral foundation detection is crucial for analyzing social discourse and developing ethically-aligned AI systems. While large language models excel across diverse tasks, their performance on specialized moral reasoning remains unclear.   This study provides the first comprehensive comparison between state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit datasets using ROC, PR, and DET curve analysis.   Results reveal substantial performance gaps, with LLMs exhibiting high false negative rates and systematic under-detection of moral content despite prompt engineering efforts. These findings demonstrate that task-specific fine-tuning remains superior to prompting for moral reasoning applications.", "published": "2025-07-24T15:49:06+00:00"}
{"id": "2507.18518v1", "title": "Transform Before You Query: A Privacy-Preserving Approach for Vector Retrieval with Embedding Space Alignment", "authors": ["Ruiqi He", "Zekun Fei", "Jiaqi Li", "Xinyuan Zhu", "Biao Yi", "Siyi Lv", "Weijie Liu", "Zheli Liu"], "categories": ["cs.IR"], "abstract": "Vector Database (VDB) can efficiently index and search high-dimensional vector embeddings from unstructured data, crucially enabling fast semantic similarity search essential for modern AI applications like generative AI and recommendation systems. Since current VDB service providers predominantly use proprietary black-box models, users are forced to expose raw query text to them via API in exchange for the vector retrieval services. Consequently, if query text involves confidential records from finance or healthcare domains, this mechanism inevitably leads to critical leakage of user's sensitive information. To address this issue, we introduce STEER (\\textbf{S}ecure \\textbf{T}ransformed \\textbf{E}mbedding v\\textbf{E}ctor\\textbf{ R}etrieval), a private vector retrieval framework that leverages the alignment relationship between the semantic spaces of different embedding models to derive approximate embeddings for the query text. STEER performs the retrieval using the approximate embeddings within the original VDB and requires no modifications to the server side. Our theoretical and experimental analyses demonstrate that STEER effectively safeguards query text privacy while maintaining the retrieval accuracy. Even though approximate embeddings are approximations of the embeddings from proprietary models, they still prevent the providers from recovering the query text through Embedding Inversion Attacks (EIAs). Extensive experimental results show that Recall@100 of STEER can basically achieve a decrease of less than 5\\%. Furthermore, even when searching within a text corpus of millions of entries, STEER achieves a Recall@20 accuracy 20\\% higher than current baselines.", "published": "2025-07-24T15:41:34+00:00"}
{"id": "2507.18505v1", "title": "LSD of sample covariances of superposition of matrices with separable covariance structure", "authors": ["Javed Hazarika", "Debashis Paul"], "categories": ["math.ST", "stat.TH"], "abstract": "We study the asymptotic behavior of the spectra of matrices of the form $S_n = \\frac{1}{n}XX^*$ where $X =\\sum_{r=1}^K X_r$, where $X_r = A_r^\\frac{1}{2}Z_rB_r^\\frac{1}{2}$, $K \\in \\mathbb{N}$ and $A_r,B_r$ are sequences of positive semi-definite matrices of dimensions $p\\times p$ and $n\\times n$, respectively. We establish the existence of a limiting spectral distribution for $S_n$ by assuming that matrices $\\{A_r\\}_{r=1}^K$ are simultaneously diagonalizable and $\\{B_r\\}_{r=1}^K$ are simultaneously digaonalizable, and that the joint spectral distributions of $\\{A_r\\}_{r=1}^K$ and $\\{B_r\\}_{r=1}^K$ converge to $K$-dimensional distributions, as $p,n\\to \\infty$ such that $p/n \\to c \\in (0,\\infty)$. The LSD of $S_n$ is characterized by system of equations with unique solutions within the class of Stieltjes transforms of measures on $\\mathbb{R}_+$. These results generalize existing results on the LSD of sample covariances when the data matrices have a separable covariance structure.", "published": "2025-07-24T15:22:38+00:00"}
{"id": "2507.18467v1", "title": "Contraction, Criticality, and Capacity: A Dynamical-Systems Perspective on Echo-State Networks", "authors": ["Pradeep Singh", "Lavanya Sankaranarayanan", "Balasubramanian Raman"], "categories": ["cs.NE", "nlin.CD", "68T07, 37M25, 37N30, 41A30", "I.2.6; F.1.1; G.3"], "abstract": "Echo-State Networks (ESNs) distil a key neurobiological insight: richly recurrent but fixed circuitry combined with adaptive linear read-outs can transform temporal streams with remarkable efficiency. Yet fundamental questions about stability, memory and expressive power remain fragmented across disciplines. We present a unified, dynamical-systems treatment that weaves together functional analysis, random attractor theory and recent neuroscientific findings. First, on compact multivariate input alphabets we prove that the Echo-State Property (wash-out of initial conditions) together with global Lipschitz dynamics necessarily yields the Fading-Memory Property (geometric forgetting of remote inputs). Tight algebraic tests translate activation-specific Lipschitz constants into certified spectral-norm bounds, covering both saturating and rectifying nonlinearities. Second, employing a Stone-Weierstrass strategy we give a streamlined proof that ESNs with polynomial reservoirs and linear read-outs are dense in the Banach space of causal, time-invariant fading-memory filters, extending universality to stochastic inputs. Third, we quantify computational resources via memory-capacity spectrum, show how topology and leak rate redistribute delay-specific capacities, and link these trade-offs to Lyapunov spectra at the \\textit{edge of chaos}. Finally, casting ESNs as skew-product random dynamical systems, we establish existence of singleton pullback attractors and derive conditional Lyapunov bounds, providing a rigorous analogue to cortical criticality. The analysis yields concrete design rules-spectral radius, input gain, activation choice-grounded simultaneously in mathematics and neuroscience, and clarifies why modest-sized reservoirs often rival fully trained recurrent networks in practice.", "published": "2025-07-24T14:41:18+00:00"}
{"id": "2507.18462v1", "title": "A Novel Monte-Carlo Compressed Sensing and Dictionary Learning Method for the Efficient Path Planning of Remote Sensing Robots", "authors": ["Alghalya Al-Hajri", "Ejmen Al-Ubejdij", "Aiman Erbad", "Ali Safa"], "categories": ["cs.RO"], "abstract": "In recent years, Compressed Sensing (CS) has gained significant interest as a technique for acquiring high-resolution sensory data using fewer measurements than traditional Nyquist sampling requires. At the same time, autonomous robotic platforms such as drones and rovers have become increasingly popular tools for remote sensing and environmental monitoring tasks, including measurements of temperature, humidity, and air quality. Within this context, this paper presents, to the best of our knowledge, the first investigation into how the structure of CS measurement matrices can be exploited to design optimized sampling trajectories for robotic environmental data collection. We propose a novel Monte Carlo optimization framework that generates measurement matrices designed to minimize both the robot's traversal path length and the signal reconstruction error within the CS framework. Central to our approach is the application of Dictionary Learning (DL) to obtain a data-driven sparsifying transform, which enhances reconstruction accuracy while further reducing the number of samples that the robot needs to collect. We demonstrate the effectiveness of our method through experiments reconstructing $NO_2$ pollution maps over the Gulf region. The results indicate that our approach can reduce robot travel distance to less than $10\\%$ of a full-coverage path, while improving reconstruction accuracy by over a factor of five compared to traditional CS methods based on DCT and polynomial dictionaries, as well as by a factor of two compared to previously-proposed Informative Path Planning (IPP) methods.", "published": "2025-07-24T14:39:01+00:00"}
{"id": "2507.18455v1", "title": "LLM-based Embedders for Prior Case Retrieval", "authors": ["Damith Premasiri", "Tharindu Ranasinghe", "Ruslan Mitkov"], "categories": ["cs.IR", "cs.CL"], "abstract": "In common law systems, legal professionals such as lawyers and judges rely on precedents to build their arguments. As the volume of cases has grown massively over time, effectively retrieving prior cases has become essential. Prior case retrieval (PCR) is an information retrieval (IR) task that aims to automatically identify the most relevant court cases for a specific query from a large pool of potential candidates. While IR methods have seen several paradigm shifts over the last few years, the vast majority of PCR methods continue to rely on traditional IR methods, such as BM25. The state-of-the-art deep learning IR methods have not been successful in PCR due to two key challenges: i. Lengthy legal text limitation; when using the powerful BERT-based transformer models, there is a limit of input text lengths, which inevitably requires to shorten the input via truncation or division with a loss of legal context information. ii. Lack of legal training data; due to data privacy concerns, available PCR datasets are often limited in size, making it difficult to train deep learning-based models effectively. In this research, we address these challenges by leveraging LLM-based text embedders in PCR. LLM-based embedders support longer input lengths, and since we use them in an unsupervised manner, they do not require training data, addressing both challenges simultaneously. In this paper, we evaluate state-of-the-art LLM-based text embedders in four PCR benchmark datasets and show that they outperform BM25 and supervised transformer-based models.", "published": "2025-07-24T14:36:10+00:00"}
{"id": "2507.18451v1", "title": "Generation of Synthetic Clinical Text: A Systematic Review", "authors": ["Basel Alshaikhdeeb", "Ahmed Abdelmonem Hemedan", "Soumyabrata Ghosh", "Irina Balaur", "Venkata Satagopam"], "categories": ["cs.CL", "cs.AI"], "abstract": "Generating clinical synthetic text represents an effective solution for common clinical NLP issues like sparsity and privacy. This paper aims to conduct a systematic review on generating synthetic medical free-text by formulating quantitative analysis to three research questions concerning (i) the purpose of generation, (ii) the techniques, and (iii) the evaluation methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE, Google Scholar, and arXiv databases for publications associated with generating synthetic medical unstructured free-text. We have identified 94 relevant articles out of 1,398 collected ones. A great deal of attention has been given to the generation of synthetic medical text from 2018 onwards, where the main purpose of such a generation is towards text augmentation, assistive writing, corpus building, privacy-preserving, annotation, and usefulness. Transformer architectures were the main predominant technique used to generate the text, especially the GPTs. On the other hand, there were four main aspects of evaluation, including similarity, privacy, structure, and utility, where utility was the most frequent method used to assess the generated synthetic medical text. Although the generated synthetic medical text demonstrated a moderate possibility to act as real medical documents in different downstream NLP tasks, it has proven to be a great asset as augmented, complementary to the real documents, towards improving the accuracy and overcoming sparsity/undersampling issues. Yet, privacy is still a major issue behind generating synthetic medical text, where more human assessments are needed to check for the existence of any sensitive information. Despite that, advances in generating synthetic medical text will considerably accelerate the adoption of workflows and pipeline development, discarding the time-consuming legalities of data transfer.", "published": "2025-07-24T14:35:16+00:00"}
{"id": "2507.18448v1", "title": "Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language", "authors": ["Md Obyedullahil Mamun", "Md Adyelullahil Mamun", "Arif Ahmad", "Md. Imran Hossain Emu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2; I.7"], "abstract": "Punctuation restoration enhances the readability of text and is critical for post-processing tasks in Automatic Speech Recognition (ASR), especially for low-resource languages like Bangla. In this study, we explore the application of transformer-based models, specifically XLM-RoBERTa-large, to automatically restore punctuation in unpunctuated Bangla text. We focus on predicting four punctuation marks: period, comma, question mark, and exclamation mark across diverse text domains. To address the scarcity of annotated resources, we constructed a large, varied training corpus and applied data augmentation techniques. Our best-performing model, trained with an augmentation factor of alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the Reference set, and 90.2% on the ASR set.   Results show strong generalization to reference and ASR transcripts, demonstrating the model's effectiveness in real-world, noisy scenarios. This work establishes a strong baseline for Bangla punctuation restoration and contributes publicly available datasets and code to support future research in low-resource NLP.", "published": "2025-07-24T14:33:13+00:00"}
{"id": "2507.18444v1", "title": "DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place Recognition", "authors": ["Haiyang Jiang", "Songhao Piao", "Chao Gao", "Lei Yu", "Liguo Chen"], "categories": ["cs.CV", "cs.RO"], "abstract": "Visual Place Recognition (VPR) is crucial for robust mobile robot localization, yet it faces significant challenges in maintaining reliable performance under varying environmental conditions and viewpoints. To address this, we propose a novel framework that integrates Dual-Scale-Former (DSFormer), a Transformer-based cross-learning module, with an innovative block clustering strategy. DSFormer enhances feature representation by enabling bidirectional information transfer between dual-scale features extracted from the final two CNN layers, capturing both semantic richness and spatial details through self-attention for long-range dependencies within each scale and shared cross-attention for cross-scale learning. Complementing this, our block clustering strategy repartitions the widely used San Francisco eXtra Large (SF-XL) training dataset from multiple distinct perspectives, optimizing data organization to further bolster robustness against viewpoint variations. Together, these innovations not only yield a robust global embedding adaptable to environmental changes but also reduce the required training data volume by approximately 30\\% compared to previous partitioning methods. Comprehensive experiments demonstrate that our approach achieves state-of-the-art performance across most benchmark datasets, surpassing advanced reranking methods like DELG, Patch-NetVLAD, TransVPR, and R2Former as a global retrieval solution using 512-dim global descriptors, while significantly improving computational efficiency.", "published": "2025-07-24T14:29:30+00:00"}
{"id": "2507.18417v1", "title": "FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs", "authors": ["Giorgos Iacovides", "Wuyang Zhou", "Danilo Mandic"], "categories": ["cs.CL", "cs.LG", "q-fin.ST", "q-fin.TR"], "abstract": "Opinions expressed in online finance-related textual data are having an increasingly profound impact on trading decisions and market movements. This trend highlights the vital role of sentiment analysis as a tool for quantifying the nature and strength of such opinions. With the rapid development of Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs) have become the de facto standard for financial sentiment analysis. However, the SFT paradigm can lead to memorization of the training data and often fails to generalize to unseen samples. This is a critical limitation in financial domains, where models must adapt to previously unobserved events and the nuanced, domain-specific language of finance. To this end, we introduce FinDPO, the first finance-specific LLM framework based on post-training human preference alignment via Direct Preference Optimization (DPO). The proposed FinDPO achieves state-of-the-art performance on standard sentiment classification benchmarks, outperforming existing supervised fine-tuned models by 11% on the average. Uniquely, the FinDPO framework enables the integration of a fine-tuned causal LLM into realistic portfolio strategies through a novel 'logit-to-score' conversion, which transforms discrete sentiment predictions into continuous, rankable sentiment scores (probabilities). In this way, simulations demonstrate that FinDPO is the first sentiment-based approach to maintain substantial positive returns of 67% annually and strong risk-adjusted performance, as indicated by a Sharpe ratio of 2.0, even under realistic transaction costs of 5 basis points (bps).", "published": "2025-07-24T13:57:05+00:00"}
{"id": "2507.18409v1", "title": "A new approach to the Monge-Amp\u00e8re eigenvalue problem", "authors": ["Chinh H. Lu", "Ahmed Zeriahi"], "categories": ["math.CV", "math.AP", "31C45, 2U15, 32U40, 32W20, 35J66, 35J96"], "abstract": "We study the eigenvalue problem for the complex Monge-Amp\\`ere operator in bounded hyperconvex domains in $\\C^n$, where the right-hand side is a non-pluripolar positive Borel measure. We establish the uniqueness of eigenfunctions in the finite energy class introduced by Cegrell, up to positive multiplicative constants, and provide a Rayleigh quotient type formula for computing the eigenvalue.   Under a natural continuity assumption on the measure, we further show that both the eigenvalue and eigenfunctions can be obtained via an iterative procedure starting from any negative finite energy function.   Our approach relies on the fine properties of plurisubharmonic envelopes, which allow a partial sublinearization of the nonlinear problem. As far as we know, this method is new, even in the linear case, and not only yields new results but also significantly simplifies existing arguments in the literature. Moreover, it extends naturally to the setting of complex Hessian operators.   Finally, by translating our results from the complex Monge-Amp\\`ere setting via a logarithmic transformation, we also obtain several interesting analogues for the real Monge-Amp\\`ere operator.", "published": "2025-07-24T13:47:52+00:00"}
{"id": "2507.18405v1", "title": "Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows", "authors": ["Simin Huo", "Ning Li"], "categories": ["cs.CV", "cs.LG"], "abstract": "We introduce Iwin Transformer, a novel position-embedding-free hierarchical vision transformer, which can be fine-tuned directly from low to high resolution, through the collaboration of innovative interleaved window attention and depthwise separable convolution. This approach uses attention to connect distant tokens and applies convolution to link neighboring tokens, enabling global information exchange within a single module, overcoming Swin Transformer's limitation of requiring two consecutive blocks to approximate global attention. Extensive experiments on visual benchmarks demonstrate that Iwin Transformer exhibits strong competitiveness in tasks such as image classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and video action recognition. We also validate the effectiveness of the core component in Iwin as a standalone module that can seamlessly replace the self-attention module in class-conditional image generation. The concepts and methods introduced by the Iwin Transformer have the potential to inspire future research, like Iwin 3D Attention in video generation. The code and models are available at https://github.com/cominder/Iwin-Transformer.", "published": "2025-07-24T13:45:48+00:00"}
{"id": "2507.18356v1", "title": "Nucleation of magnetic textures in stripe domain bifurcations for reconfigurable domain wall racetracks", "authors": ["V. V. Fern\u00e1ndez", "S. Ferrer", "A. Hierro-Rodr\u00edguez", "M. V\u00e9lez"], "categories": ["cond-mat.mtrl-sci", "physics.app-ph"], "abstract": "Within the racetrack memory paradigm, systems exploiting magnetic guiding potentials instead of geometrical ones, allow for enhancing the versatility of the final devices adding magnetic reconfigurable capabilities. Hard/soft magnetic multilayers with stripe domain configurations fulfill these requirements. In these systems, the topology of the generated textures that would act as information carriers, is strongly conditioned by the stripe lattice configuration. Micromagnetic simulations have been used to study the magnetization reversal process in NdCo$_5$/Py reconfigurable racetracks. By using skyrmionic charges and magnetic vorticity lines, the topological transformations controlling the nucleation of vortices, antivortices, Bloch lines and Bloch points has been analyzed. It has been shown that magnetic topological charge exchanges between textures rule the formation of vortex/antivortex pairs with opposite polarities, key for the guided propagation through the stripe pattern.", "published": "2025-07-24T12:26:35+00:00"}
{"id": "2507.18347v1", "title": "The PyKOALA python library: a multi-instrument package for IFS data reduction", "authors": ["Pablo Corcho-Caballero", "Yago Ascasibar", "\u00c1ngel R. L\u00f3pez-S\u00e1nchez", "Miguel Gonz\u00e1lez-Bolivar", "Nuria P. F. Lorente", "James Tocknell", "Felipe Jim\u00e9nez-Ibarra", "Praveen Jayasuriya Daluwathumullagamage", "Gabriella Quattropani", "Matt Owers", "Gijs A. Verdoes-Kleijn"], "categories": ["astro-ph.IM", "astro-ph.GA"], "abstract": "PyKOALA is an innovative Python-based library designed to provide a robust and flexible framework for Integral Field Spectroscopy (IFS) data reduction. By addressing the complexities of transforming raw measurements into scientifically valuable spectra, PyKOALA simplifies the data reduction pipeline while remaining instrument-agnostic and user-friendly. This proceeding outlines the challenges of IFS data reduction, PyKOALA's architecture, and its applications to observations by the KOALA+AAOmega instruments at the Anglo-Australian Telescope.", "published": "2025-07-24T12:19:38+00:00"}
{"id": "2507.18332v1", "title": "Hierarchical Dimensionless Learning (Hi-\u03c0): A physics-data hybrid-driven approach for discovering dimensionless parameter combinations", "authors": ["Mingkun Xia", "Haitao Lin", "Weiwei Zhang"], "categories": ["physics.flu-dyn", "cs.LG", "physics.data-an"], "abstract": "Dimensional analysis provides a universal framework for reducing physical complexity and reveal inherent laws. However, its application to high-dimensional systems still generates redundant dimensionless parameters, making it challenging to establish physically meaningful descriptions. Here, we introduce Hierarchical Dimensionless Learning (Hi-{\\pi}), a physics-data hybrid-driven method that combines dimensional analysis and symbolic regression to automatically discover key dimensionless parameter combination(s). We applied this method to classic examples in various research fields of fluid mechanics. For the Rayleigh-B\\'enard convection, this method accurately extracted two intrinsic dimensionless parameters: the Rayleigh number and the Prandtl number, validating its unified representation advantage across multiscale data. For the viscous flows in a circular pipe, the method automatically discovers two optimal dimensionless parameters: the Reynolds number and relative roughness, achieving a balance between accuracy and complexity. For the compressibility correction in subsonic flow, the method effectively extracts the classic compressibility correction formulation, while demonstrating its capability to discover hierarchical structural expressions through optimal parameter transformations.", "published": "2025-07-24T11:59:10+00:00"}
{"id": "2507.18329v1", "title": "Transfer using Fourier transform and minimal representation of $E_7$", "authors": ["Nhat Hoang Le", "Bryan Peng Jun Wang"], "categories": ["math.RT", "math.NT"], "abstract": "In this paper, we study the Sakellaridis-Venkatesh conjecture for the rank-1 spherical variety $X=\\text{Spin}_9\\backslash F_4$ using an exceptional theta correspondence. We establish the correct transfer map satisfying relative character identities in this case and show that our transfer map agrees with the formula in (Sakellaridis, 2021). Moreover, we show how our techniques lead to a characterization of $X$-relatively cuspidal representations.", "published": "2025-07-24T11:56:54+00:00"}
{"id": "2507.18327v1", "title": "Beyond Low-rankness: Guaranteed Matrix Recovery via Modified Nuclear Norm", "authors": ["Jiangjun Peng", "Yisi Luo", "Xiangyong Cao", "Shuang Xu", "Deyu Meng"], "categories": ["cs.CV"], "abstract": "The nuclear norm (NN) has been widely explored in matrix recovery problems, such as Robust PCA and matrix completion, leveraging the inherent global low-rank structure of the data. In this study, we introduce a new modified nuclear norm (MNN) framework, where the MNN family norms are defined by adopting suitable transformations and performing the NN on the transformed matrix. The MNN framework offers two main advantages: (1) it jointly captures both local information and global low-rankness without requiring trade-off parameter tuning; (2) Under mild assumptions on the transformation, we provided exact theoretical recovery guarantees for both Robust PCA and MC tasks-an achievement not shared by existing methods that combine local and global information. Thanks to its general and flexible design, MNN can accommodate various proven transformations, enabling a unified and effective approach to structured low-rank recovery. Extensive experiments demonstrate the effectiveness of our method. Code and supplementary material are available at https://github.com/andrew-pengjj/modified_nuclear_norm.", "published": "2025-07-24T11:53:55+00:00"}
{"id": "2507.18323v1", "title": "A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation", "authors": ["Minje Park", "Jeonghwa Lim", "Taehyung Yu", "Sunghoon Joo"], "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.SP"], "abstract": "Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform features, is critical for clinical diagnosis. Despite recent advances using deep learning, progress has been limited by the scarcity of publicly available annotated datasets. Semi-supervised learning presents a promising solution by leveraging abundant unlabeled ECG data. In this study, we present the first systematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG delineation. We curated and unified multiple public datasets, including previously underused sources, to support robust and diverse evaluation. We adopted five representative SemiSeg algorithms from computer vision, implemented them on two different architectures: the convolutional network and the transformer, and evaluated them in two different settings: in-domain and cross-domain. Additionally, we propose ECG-specific training configurations and augmentation strategies and introduce a standardized evaluation framework. Our results show that the transformer outperforms the convolutional network in semi-supervised ECG delineation. We anticipate that our benchmark will serve as a foundation for advancing semi-supervised ECG delineation methods and will facilitate further research in this domain.", "published": "2025-07-24T11:49:46+00:00"}
{"id": "2507.18320v1", "title": "State of Health Estimation of Batteries Using a Time-Informed Dynamic Sequence-Inverted Transformer", "authors": ["Janak M. Patel", "Milad Ramezankhani", "Anirudh Deodhar", "Dagnachew Birru"], "categories": ["cs.LG"], "abstract": "The rapid adoption of battery-powered vehicles and energy storage systems over the past decade has made battery health monitoring increasingly critical. Batteries play a central role in the efficiency and safety of these systems, yet they inevitably degrade over time due to repeated charge-discharge cycles. This degradation leads to reduced energy efficiency and potential overheating, posing significant safety concerns. Accurate estimation of a State of Health (SoH) of battery is therefore essential for ensuring operational reliability and safety. Several machine learning architectures, such as LSTMs, transformers, and encoder-based models, have been proposed to estimate SoH from discharge cycle data. However, these models struggle with the irregularities inherent in real-world measurements: discharge readings are often recorded at non-uniform intervals, and the lengths of discharge cycles vary significantly. To address this, most existing approaches extract features from the sequences rather than processing them in full, which introduces information loss and compromises accuracy. To overcome these challenges, we propose a novel architecture: Time-Informed Dynamic Sequence Inverted Transformer (TIDSIT). TIDSIT incorporates continuous time embeddings to effectively represent irregularly sampled data and utilizes padded sequences with temporal attention mechanisms to manage variable-length inputs without discarding sequence information. Experimental results on the NASA battery degradation dataset show that TIDSIT significantly outperforms existing models, achieving over 50% reduction in prediction error and maintaining an SoH prediction error below 0.58%. Furthermore, the architecture is generalizable and holds promise for broader applications in health monitoring tasks involving irregular time-series data.", "published": "2025-07-24T11:43:46+00:00"}
{"id": "2507.18303v1", "title": "The Alpha Group Dynamic Mapping", "authors": ["Cleber Souza Corr\u00eaa", "Thiago Braido Nogueira de Melo"], "categories": ["math.DG", "math.AG", "math.DS", "53C99, 57R20, 37D45"], "abstract": "This paper investigates the dynamical behavior of a system of ordinary differential equations (ODEs) governed by a matrix that represents the division in the algebra of the Alpha group. As the system evolves, the matrix induces topological transitions in geometric spaces, controlled by a rotational parameter. Numerical simulations are performed using a fourth-order Runge-Kutta method implemented in Python. The results reveal the emergence of topological nodes, the existence of critical points at which the rotation between dividing planes transitions from 0 to $\\pi/2$ radians. Near zero radians, the system exhibits a Euclidean geometric structure, while rotations close to $\\pi/2$ define an Alpha Group space. At these nodes, the matrix-driven ODE system undergoes qualitative dynamic changes, reflecting distinct topological behaviors. The Alpha Group matrix is interpreted as a generator of symmetry transformations, potentially analogous to gauge fields under local or global symmetries. This work provides a computational framework for exploring dynamic topologies, attractors at infinity, and internal coherence in hyper-complex vector spaces.", "published": "2025-07-24T11:21:32+00:00"}
{"id": "2507.18293v1", "title": "Leveraging Data Augmentation and Siamese Learning for Predictive Process Monitoring", "authors": ["Sjoerd van Straten", "Alessandro Padella", "Marwan Hassani"], "categories": ["cs.LG"], "abstract": "Predictive Process Monitoring (PPM) enables forecasting future events or outcomes of ongoing business process instances based on event logs. However, deep learning PPM approaches are often limited by the low variability and small size of real-world event logs. To address this, we introduce SiamSA-PPM, a novel self-supervised learning framework that combines Siamese learning with Statistical Augmentation for Predictive Process Monitoring. It employs three novel statistically grounded transformation methods that leverage control-flow semantics and frequent behavioral patterns to generate realistic, semantically valid new trace variants. These augmented views are used within a Siamese learning setup to learn generalizable representations of process prefixes without the need for labeled supervision. Extensive experiments on real-life event logs demonstrate that SiamSA-PPM achieves competitive or superior performance compared to the SOTA in both next activity and final outcome prediction tasks. Our results further show that statistical augmentation significantly outperforms random transformations and improves variability in the data, highlighting SiamSA-PPM as a promising direction for training data enrichment in process prediction.", "published": "2025-07-24T10:57:20+00:00"}
{"id": "2507.18246v1", "title": "Resourceful Traces for Commuting Processes", "authors": ["Matthew Earnshaw", "Chad Nester", "Mario Rom\u00e1n"], "categories": ["cs.LO", "math.CT"], "abstract": "We show that, when the actions of a Mazurkiewicz trace are considered not merely as atomic (i.e., mere names) but transformations from a specified type of inputs to a specified type of outputs, we obtain a novel notion of presentation for effectful categories (also known as generalised Freyd categories), a well-known algebraic structure in the semantics of side-effecting computation. Like the usual representation of traces as graphs, our notion of presentation gives rise to a graphical calculus for effectful categories. We use our presentations to give a construction of the commuting tensor product of free effectful categories, capturing the combination of systems in which the actions of each must commute with one another, while still permitting exchange of resources", "published": "2025-07-24T09:35:44+00:00"}
{"id": "2507.18225v1", "title": "3D Test-time Adaptation via Graph Spectral Driven Point Shift", "authors": ["Xin Wei", "Qin Yang", "Yijie Fang", "Mingrui Zhu", "Nannan Wang"], "categories": ["cs.CV"], "abstract": "While test-time adaptation (TTA) methods effectively address domain shifts by dynamically adapting pre-trained models to target domain data during online inference, their application to 3D point clouds is hindered by their irregular and unordered structure. Current 3D TTA methods often rely on computationally expensive spatial-domain optimizations and may require additional training data. In contrast, we propose Graph Spectral Domain Test-Time Adaptation (GSDTTA), a novel approach for 3D point cloud classification that shifts adaptation to the graph spectral domain, enabling more efficient adaptation by capturing global structural properties with fewer parameters. Point clouds in target domain are represented as outlier-aware graphs and transformed into graph spectral domain by Graph Fourier Transform (GFT). For efficiency, adaptation is performed by optimizing only the lowest 10% of frequency components, which capture the majority of the point cloud's energy. An inverse GFT (IGFT) is then applied to reconstruct the adapted point cloud with the graph spectral-driven point shift. This process is enhanced by an eigenmap-guided self-training strategy that iteratively refines both the spectral adjustments and the model parameters. Experimental results and ablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA, outperforming existing TTA methods for 3D point cloud classification.", "published": "2025-07-24T09:18:39+00:00"}
{"id": "2507.18215v1", "title": "Information Security Based on LLM Approaches: A Review", "authors": ["Chang Gong", "Zhongwen Li", "Xiaoqi Li"], "categories": ["cs.CR", "cs.AI"], "abstract": "Information security is facing increasingly severe challenges, and traditional protection means are difficult to cope with complex and changing threats. In recent years, as an emerging intelligent technology, large language models (LLMs) have shown a broad application prospect in the field of information security. In this paper, we focus on the key role of LLM in information security, systematically review its application progress in malicious behavior prediction, network threat analysis, system vulnerability detection, malicious code identification, and cryptographic algorithm optimization, and explore its potential in enhancing security protection performance. Based on neural networks and Transformer architecture, this paper analyzes the technical basis of large language models and their advantages in natural language processing tasks. It is shown that the introduction of large language modeling helps to improve the detection accuracy and reduce the false alarm rate of security systems. Finally, this paper summarizes the current application results and points out that it still faces challenges in model transparency, interpretability, and scene adaptability, among other issues. It is necessary to explore further the optimization of the model structure and the improvement of the generalization ability to realize a more intelligent and accurate information security protection system.", "published": "2025-07-24T09:09:36+00:00"}
{"id": "2507.18214v1", "title": "LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned Features in Medical Image Segmentation", "authors": ["Qilin Huang", "Tianyu Lin", "Zhiguang Chen", "Fudan Zheng"], "categories": ["cs.CV"], "abstract": "Leveraging the powerful capabilities of diffusion models has yielded quite effective results in medical image segmentation tasks. However, existing methods typically transfer the original training process directly without specific adjustments for segmentation tasks. Furthermore, the commonly used pre-trained diffusion models still have deficiencies in feature extraction. Based on these considerations, we propose LEAF, a medical image segmentation model grounded in latent diffusion models. During the fine-tuning process, we replace the original noise prediction pattern with a direct prediction of the segmentation map, thereby reducing the variance of segmentation results. We also employ a feature distillation method to align the hidden states of the convolutional layers with the features from a transformer-based vision encoder. Experimental results demonstrate that our method enhances the performance of the original diffusion model across multiple segmentation datasets for different disease types. Notably, our approach does not alter the model architecture, nor does it increase the number of parameters or computation during the inference phase, making it highly efficient.", "published": "2025-07-24T09:08:04+00:00"}
{"id": "2507.18197v1", "title": "Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization", "authors": ["Aline Belloni", "Patrick Prieur"], "categories": ["cs.CL", "cs.DL"], "abstract": "Business process modeling is used by most organizations as an essential framework for ensuring efficiency and effectiveness of the work and workflow performed by its employees and for ensuring the alignment of such work with its strategic goals. For organizations that are compliant or near-compliant with ISO 9001, this approach involves the detailed mapping of processes, sub-processes, activities, and tasks. ISO30401 is a Management System Standard, introduced in 2018, establishing universal requirements for the set up of a Knowledge Management System in an organization. As ``ISO30401 implementers'' we regularly face the challenge of explaining our clients how the knowledge development, transformation and conveyances activities depicted in ISO30401 do integrate with existing operational processes. This article recaps process modelling principles in the context of ISO9001 and explores, based on our experience, how an ISO30401-compliant Knowledge Management System (KMS) entwines with all other processes of an Integrated Management System and in particular how it can be implemented by deploying the mechanisms of the SECI model through the steps of PDCA cycles.", "published": "2025-07-24T08:54:19+00:00"}
{"id": "2507.18173v1", "title": "WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection", "authors": ["Haodong Zhu", "Wenhao Dong", "Linlin Yang", "Hong Li", "Yuguang Yang", "Yangyang Ren", "Qingcheng Zhu", "Zichao Feng", "Changbai Li", "Shaohui Lin", "Runqi Wang", "Xiaoyan Luo", "Baochang Zhang"], "categories": ["cs.CV", "cs.MM"], "abstract": "Leveraging the complementary characteristics of visible (RGB) and infrared (IR) imagery offers significant potential for improving object detection. In this paper, we propose WaveMamba, a cross-modality fusion method that efficiently integrates the unique and complementary frequency features of RGB and IR decomposed by Discrete Wavelet Transform (DWT). An improved detection head incorporating the Inverse Discrete Wavelet Transform (IDWT) is also proposed to reduce information loss and produce the final detection results. The core of our approach is the introduction of WaveMamba Fusion Block (WMFB), which facilitates comprehensive fusion across low-/high-frequency sub-bands. Within WMFB, the Low-frequency Mamba Fusion Block (LMFB), built upon the Mamba framework, first performs initial low-frequency feature fusion with channel swapping, followed by deep fusion with an advanced gated attention mechanism for enhanced integration. High-frequency features are enhanced using a strategy that applies an ``absolute maximum\" fusion approach. These advancements lead to significant performance gains, with our method surpassing state-of-the-art approaches and achieving average mAP improvements of 4.5% on four benchmarks.", "published": "2025-07-24T08:16:15+00:00"}
{"id": "2507.18171v1", "title": "Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models", "authors": ["Kexin Chen", "Dongxia Wang", "Yi Liu", "Haonan Zhang", "Wenhai Wang"], "categories": ["cs.CL", "cs.AI"], "abstract": "Despite the widespread use of Transformer-based text embedding models in NLP tasks, surprising 'sticky tokens' can undermine the reliability of embeddings. These tokens, when repeatedly inserted into sentences, pull sentence similarity toward a certain value, disrupting the normal distribution of embedding distances and degrading downstream performance. In this paper, we systematically investigate such anomalous tokens, formally defining them and introducing an efficient detection method, Sticky Token Detector (STD), based on sentence and token filtering. Applying STD to 40 checkpoints across 14 model families, we discover a total of 868 sticky tokens. Our analysis reveals that these tokens often originate from special or unused entries in the vocabulary, as well as fragmented subwords from multilingual corpora. Notably, their presence does not strictly correlate with model size or vocabulary size. We further evaluate how sticky tokens affect downstream tasks like clustering and retrieval, observing significant performance drops of up to 50%. Through attention-layer analysis, we show that sticky tokens disproportionately dominate the model's internal representations, raising concerns about tokenization robustness. Our findings show the need for better tokenization strategies and model design to mitigate the impact of sticky tokens in future text embedding applications.", "published": "2025-07-24T08:13:16+00:00"}
{"id": "2507.18170v1", "title": "Trek-Based Parameter Identification for Linear Causal Models With Arbitrarily Structured Latent Variables", "authors": ["Nils Sturma", "Mathias Drton"], "categories": ["math.ST", "stat.ML", "stat.TH"], "abstract": "We develop a criterion to certify whether causal effects are identifiable in linear structural equation models with latent variables. Linear structural equation models correspond to directed graphs whose nodes represent the random variables of interest and whose edges are weighted with linear coefficients that correspond to direct causal effects. In contrast to previous identification methods, we do not restrict ourselves to settings where the latent variables constitute independent latent factors (i.e., to source nodes in the graphical representation of the model). Our novel latent-subgraph criterion is a purely graphical condition that is sufficient for identifiability of causal effects by rational formulas in the covariance matrix. To check the latent-subgraph criterion, we provide a sound and complete algorithm that operates by solving an integer linear program. While it targets effects involving observed variables, our new criterion is also useful for identifying effects between latent variables, as it allows one to transform the given model into a simpler measurement model for which other existing tools become applicable.", "published": "2025-07-24T08:10:44+00:00"}
{"id": "2507.18135v1", "title": "Information Entropy-Based Framework for Quantifying Tortuosity in Meibomian Gland Uneven Atrophy", "authors": ["Kesheng Wang", "Xiaoyu Chen", "Chunlei He", "Fenfen Li", "Xinxin Yu", "Dexing Kong", "Shoujun Huang", "Qi Dai"], "categories": ["cs.CV", "cs.IT", "math.IT"], "abstract": "In the medical image analysis field, precise quantification of curve tortuosity plays a critical role in the auxiliary diagnosis and pathological assessment of various diseases. In this study, we propose a novel framework for tortuosity quantification and demonstrate its effectiveness through the evaluation of meibomian gland atrophy uniformity,serving as a representative application scenario.   We introduce an information entropy-based tortuosity quantification framework that integrates probability modeling with entropy theory and incorporates domain transformation of curve data. Unlike traditional methods such as curvature or arc-chord ratio, this approach evaluates the tortuosity of a target curve by comparing it to a designated reference curve. Consequently, it is more suitable for tortuosity assessment tasks in medical data where biologically plausible reference curves are available, providing a more robust and objective evaluation metric without relying on idealized straight-line comparisons.   First, we conducted numerical simulation experiments to preliminarily assess the stability and validity of the method. Subsequently, the framework was applied to quantify the spatial uniformity of meibomian gland atrophy and to analyze the difference in this uniformity between \\textit{Demodex}-negative and \\textit{Demodex}-positive patient groups. The results demonstrated a significant difference in tortuosity-based uniformity between the two groups, with an area under the curve of 0.8768, sensitivity of 0.75, and specificity of 0.93. These findings highlight the clinical utility of the proposed framework in curve tortuosity analysis and its potential as a generalizable tool for quantitative morphological evaluation in medical diagnostics.", "published": "2025-07-24T06:51:10+00:00"}
{"id": "2507.18104v1", "title": "A Multimodal Seq2Seq Transformer for Predicting Brain Responses to Naturalistic Stimuli", "authors": ["Qianyi He", "Yuan Chang Leong"], "categories": ["cs.CV", "q-bio.NC"], "abstract": "The Algonauts 2025 Challenge called on the community to develop encoding models that predict whole-brain fMRI responses to naturalistic multimodal movies. In this submission, we propose a sequence-to-sequence Transformer that autoregressively predicts fMRI activity from visual, auditory, and language inputs. Stimulus features were extracted using pretrained models including VideoMAE, HuBERT, Qwen, and BridgeTower. The decoder integrates information from prior brain states, current stimuli, and episode-level summaries via dual cross-attention mechanisms that attend to both perceptual information extracted from the stimulus as well as narrative information provided by high-level summaries of narrative content. One core innovation of our approach is the use of sequences of multimodal context to predict sequences of brain activity, enabling the model to capture long-range temporal structure in both stimuli and neural responses. Another is the combination of a shared encoder with partial subject-specific decoder, which leverages common structure across subjects while accounting for individual variability. Our model achieves strong performance on both in-distribution and out-of-distribution data, demonstrating the effectiveness of temporally-aware, multimodal sequence modeling for brain activity prediction. The code is available at https://github.com/Angelneer926/Algonauts_challenge.", "published": "2025-07-24T05:29:37+00:00"}
{"id": "2507.18076v1", "title": "Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints", "authors": ["Haomin Qi", "Zihan Dai", "Chengbo Huang"], "categories": ["cs.CL"], "abstract": "Fine-tuning large language models (LLMs) remains a computational bottleneck due to their scale and memory demands. This paper presents a comprehensive evaluation of parameter-efficient fine-tuning (PEFT) techniques, including LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that dynamically integrates BOFT's orthogonal stability with LoRA-GA's gradient-aligned rapid convergence. By computing per-layer adaptive updates guided by gradient norms, the hybrid method achieves superior convergence efficiency and generalization across diverse tasks. We also explore, for the first time, the adaptation of unitary RNN (uRNN) principles to transformer-based LLMs, enhancing gradient stability through structured unitary constraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench, and HumanEval -- using models ranging from 7B to 405B parameters demonstrate that our hybrid method consistently outperforms individual PEFT baselines, approaching full fine-tuning accuracy while reducing resource consumption by up to 2.1 times in training time and 50 percent in memory usage. These findings establish the hybrid approach as a practical and scalable fine-tuning solution for real-world deployment of LLMs under resource constraints.", "published": "2025-07-24T04:00:02+00:00"}
{"id": "2507.18074v1", "title": "AlphaGo Moment for Model Architecture Discovery", "authors": ["Yixiu Liu", "Yang Nan", "Weixian Xu", "Xiangkun Hu", "Lyumanshan Ye", "Zhen Qin", "Pengfei Liu"], "categories": ["cs.AI"], "abstract": "While AI systems demonstrate exponentially improving capabilities, the pace of AI research itself remains linearly bounded by human cognitive capacity, creating an increasingly severe development bottleneck. We present ASI-Arch, the first demonstration of Artificial Superintelligence for AI research (ASI4AI) in the critical domain of neural architecture discovery--a fully autonomous system that shatters this fundamental constraint by enabling AI to conduct its own architectural innovation. Moving beyond traditional Neural Architecture Search (NAS), which is fundamentally limited to exploring human-defined spaces, we introduce a paradigm shift from automated optimization to automated innovation. ASI-Arch can conduct end-to-end scientific research in the domain of architecture discovery, autonomously hypothesizing novel architectural concepts, implementing them as executable code, training and empirically validating their performance through rigorous experimentation and past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000 GPU hours, culminating in the discovery of 106 innovative, state-of-the-art (SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed unexpected strategic insights invisible to human players, our AI-discovered architectures demonstrate emergent design principles that systematically surpass human-designed baselines and illuminate previously unknown pathways for architectural innovation. Crucially, we establish the first empirical scaling law for scientific discovery itself--demonstrating that architectural breakthroughs can be scaled computationally, transforming research progress from a human-limited to a computation-scalable process. We provide comprehensive analysis of the emergent design patterns and autonomous research capabilities that enabled these breakthroughs, establishing a blueprint for self-accelerating AI systems.", "published": "2025-07-24T03:57:27+00:00"}
{"id": "2507.18043v1", "title": "GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs", "authors": ["Duy Nguyen", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "abstract": "Inference-time steering methods offer a lightweight alternative to fine-tuning large language models (LLMs) and vision-language models (VLMs) by modifying internal activations at test time without updating model weights. However, most existing approaches rely on fixed, global intervention vectors, overlook the causal influence of individual input tokens, and fail to leverage informative gradients from the model's logits, particularly in multimodal settings where visual and textual inputs contribute unevenly. To address these limitations, we introduce GrAInS, an inference-time steering approach that operates across both language-only and vision-language models and tasks. GrAInS uses contrastive, gradient-based attribution via Integrated Gradients to identify the top-k most influential tokens, both positively and negatively attributed based on their contribution to preferred versus dispreferred outputs. These tokens are then used to construct directional steering vectors that capture semantic shifts from undesirable to desirable behavior. During inference, GrAInS adjusts hidden activations at transformer layers guided by token-level attribution signals, and normalizes activations to preserve representational scale. This enables fine-grained, interpretable, and modular control over model behavior, without retraining or auxiliary supervision. Empirically, GrAInS consistently outperforms both fine-tuning and existing steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514 with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all while preserving the model's fluency and general capabilities.", "published": "2025-07-24T02:34:13+00:00"}
{"id": "2507.18025v1", "title": "A Novel Coded Computing Approach for Distributed Multi-Task Learning", "authors": ["Minquan Cheng", "Yongkang Wang", "Lingyu Zhang", "Youlong Wu"], "categories": ["cs.IT", "math.IT"], "abstract": "Distributed multi-task learning (DMTL) effectively improves model generalization performance through the collaborative training of multiple related models. However, in large-scale learning scenarios, communication bottlenecks severely limit practical system performance. In this paper, we investigate the communication bottleneck within a typical DMTL system that employs non-linear global updates. This system involves distributed workers, assisted by a central server, who collaboratively learn distinct models derived from a non-linear aggregation of their local model parameters. We first characterize the communication process as a matrix decomposition problem. It transforms workers' data storage constraints into structural characteristics of the uplink encoding matrix, and worker data retrieval demands into Maximum Distance Separable (MDS) properties of the downlink encoding matrix. Building on this, we propose a novel coded DTML scheme that can greatly reduce the communication cost of the DTML with heterogeneous data placement. Theoretical analysis demonstrates that the proposed scheme achieves the theoretical lower bound for communication overhead under mild conditions. Remarkably, this optimality holds for both traditional homogeneous computing environments and various heterogeneous scenarios. Furthermore, our scheme is extensible to a distributed linearly separable computation problem where the target function involves multiple linear combinations of local update values. This indicates that our scheme offers a new way of tackling heterogeneous data placement challenges in various distributed applications.", "published": "2025-07-24T01:55:15+00:00"}
{"id": "2507.18013v1", "title": "Technical Report of TeleChat2, TeleChat2.5 and T1", "authors": ["Zihan Wang", "Xinzhang Liu", "Yitong Yao", "Chao Wang", "Yu Zhao", "Zhihao Yang", "Wenmin Deng", "Kaipeng Jia", "Jiaxin Peng", "Yuyao Huang", "Sishi Xiong", "Zhuo Jiang", "Kaidong Yu", "Xiaohui Hu", "Fubei Yao", "Ruiyu Fang", "Zhuoru Jiang", "Ruiting Song", "Qiyi Xie", "Rui Xue", "Xuewei He", "Yanlei Xue", "Zhu Yuan", "Zhaoxi Zhang", "Zilu Huang", "Shiquan Wang", "Xin Wang", "Hanming Wu", "Mingyuan Wang", "Xufeng Zhan", "Yuhan Sun", "Zhaohu Xing", "Yuhao Jiang", "Bingkai Yang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "categories": ["cs.CL", "I.2.7"], "abstract": "We introduce the latest series of TeleChat models: \\textbf{TeleChat2}, \\textbf{TeleChat2.5}, and \\textbf{T1}, offering a significant upgrade over their predecessor, TeleChat. Despite minimal changes to the model architecture, the new series achieves substantial performance gains through enhanced training strategies in both pre-training and post-training stages. The series begins with \\textbf{TeleChat2}, which undergoes pretraining on 10 trillion high-quality and diverse tokens. This is followed by Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to further enhance its capabilities. \\textbf{TeleChat2.5} and \\textbf{T1} expand the pipeline by incorporating a continual pretraining phase with domain-specific datasets, combined with reinforcement learning (RL) to improve performance in code generation and mathematical reasoning tasks. The \\textbf{T1} variant is designed for complex reasoning, supporting long Chain-of-Thought (CoT) reasoning and demonstrating substantial improvements in mathematics and coding. In contrast, \\textbf{TeleChat2.5} prioritizes speed, delivering rapid inference. Both flagship models of \\textbf{T1} and \\textbf{TeleChat2.5} are dense Transformer-based architectures with 115B parameters, showcasing significant advancements in reasoning and general task performance compared to the original TeleChat. Notably, \\textbf{T1-115B} outperform proprietary models such as OpenAI's o1-mini and GPT-4o. We publicly release \\textbf{TeleChat2}, \\textbf{TeleChat2.5} and \\textbf{T1}, including post-trained versions with 35B and 115B parameters, to empower developers and researchers with state-of-the-art language models tailored for diverse applications.", "published": "2025-07-24T01:00:48+00:00"}
{"id": "2507.18009v1", "title": "GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures", "authors": ["Jake R. Patock", "Nicole Catherine Lewis", "Kevin McCoy", "Christina Gomez", "Canling Chen", "Lorenzo Luzi"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "State-of-the-art (SOTA) image and text generation models are multimodal models that have many similarities to large language models (LLMs). Despite achieving strong performances, leading foundational multimodal model architectures frequently lag behind the architectural sophistication of contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner (CoCa) model that incorporates Gaussian error gated linear units, root mean squared normalization, and rotary positional embedding into the textual decoders and the vision transformer (ViT) encoder. Each architectural modification has been shown to improve model performance in LLMs, but has yet to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model with the same modified textual decoders but with CoCa's original ViT encoder. We used standard pretraining and fine-tuning workflows to benchmark the models on contrastive and generative tasks. Our GRR-CoCa significantly outperformed Baseline CoCa on the pretraining dataset and three diverse fine-tuning datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were 13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We show that GRR-CoCa's modified architecture improves performance and generalization across vision-language domains.", "published": "2025-07-24T00:54:31+00:00"}
{"id": "2507.18006v1", "title": "Unlock the Potential of Fine-grained LLM Serving via Dynamic Module Scaling", "authors": ["Jingfeng Wu", "Yiyuan He", "Minxian Xu", "Xitong Gao", "Kejiang Ye", "Chengzhong Xu"], "categories": ["cs.DC"], "abstract": "The rise of large language models (LLMs) has created new opportunities across various fields but has also introduced significant challenges in resource management. Current LLM serving systems face a fundamental tension: balancing serving demands with limited resources while adapting to unpredictable traffic patterns. Static deployments lead to suboptimal resource utilization and performance degradation under dynamic workloads. Furthermore, the high cost of adjusting instances hinders dynamic scaling, limiting the true potential of efficient LLM serving.   To address this, we propose CoCoServe, an elastic system that facilitates dynamic and fine-grained scaling. Its key innovation lies in the module-level operations for the replication and migration of LLM modules, such as decoder layers and projections. Through a comprehensive analysis of the trade-offs associated with these operations, we develop an auto-scaling mechanism that dynamically regulates module-level resource allocation and performance optimization, enabling a more cost-effective deployment of LLMs. Our evaluation demonstrates that the scaling operations employed by CoCoServe exhibit excellent scalability and can reduce costs by 46% while maintaining availability. Compared to state-of-the-art LLM serving systems (e.g., Hugging Face Transformers and vLLM), our approach reduces latency by 14%-75% and achieves 1.16x-4x throughput on average across different model sizes and workloads.", "published": "2025-07-24T00:49:48+00:00"}
{"id": "2507.18005v1", "title": "C-Koordinator: Interference-aware Management for Large-scale and Co-located Microservice Clusters", "authors": ["Shengye Song", "Minxian Xu", "Zuowei Zhang", "Chengxi Gao", "Fansong Zeng", "Yu Ding", "Kejiang Ye", "Chengzhong Xu"], "categories": ["cs.DC"], "abstract": "Microservices transform traditional monolithic applications into lightweight, loosely coupled application components and have been widely adopted in many enterprises. Cloud platform infrastructure providers enhance the resource utilization efficiency of microservices systems by co-locating different microservices. However, this approach also introduces resource competition and interference among microservices. Designing interference-aware strategies for large-scale, co-located microservice clusters is crucial for enhancing resource utilization and mitigating competition-induced interference. These challenges are further exacerbated by unreliable metrics, application diversity, and node heterogeneity.   In this paper, we first analyze the characteristics of large-scale and co-located microservices clusters at Alibaba and further discuss why cycle per instruction (CPI) is adopted as a metric for interference measurement in large-scale production clusters, as well as how to achieve accurate prediction of CPI through multi-dimensional metrics. Based on CPI interference prediction and analysis, we also present the design of the C-Koordinator platform, an open-source solution utilized in Alibaba cluster, which incorporates co-location and interference mitigation strategies. The interference prediction models consistently achieve over 90.3% accuracy, enabling precise prediction and rapid mitigation of interference in operational environments. As a result, application latency is reduced and stabilized across all percentiles (P50, P90, P99) response time (RT), achieving improvements ranging from 16.7% to 36.1% under various system loads compared with state-of-the-art system. These results demonstrate the system's ability to maintain smooth application performance in co-located environments.", "published": "2025-07-24T00:49:41+00:00"}
{"id": "2507.18004v1", "title": "E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI", "authors": ["Yusen Peng", "Shuhua Mao"], "categories": ["cs.AI"], "abstract": "How can AI move beyond imitation toward genuine creativity? This paper proposes the E.A.R.T.H. framework, a five-stage generative pipeline that transforms model-generated errors into creative assets through Error generation, Amplification, Refine selection, Transform, and Harness feedback. Drawing on cognitive science and generative modeling, we posit that \"creative potential hides in failure\" and operationalize this via structured prompts, semantic scoring, and human-in-the-loop evaluation. Implemented using LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the pipeline employs a composite reward function based on novelty, surprise, and relevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to 1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4% improvement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a 4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment (CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs scored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones (3.99). Feedback highlights stylistic precision and emotional resonance. These results demonstrate that error-centered, feedback-driven generation enhances creativity, offering a scalable path toward self-evolving, human-aligned creative AI.", "published": "2025-07-24T00:39:19+00:00"}
{"id": "2507.18003v1", "title": "Black-box optimization using factorization and Ising machines", "authors": ["Ryo Tamura", "Yuya Seki", "Yuki Minamoto", "Koki Kitai", "Yoshiki Matsuda", "Shu Tanaka", "Koji Tsuda"], "categories": ["cond-mat.stat-mech", "quant-ph"], "abstract": "Black-box optimization (BBO) is used in materials design, drug discovery, and hyperparameter tuning in machine learning. The world is experiencing several of these problems. In this review, a factorization machine with quantum annealing or with quadratic-optimization annealing (FMQA) algorithm to realize fast computations of BBO using Ising machines (IMs) is discussed. The FMQA algorithm uses a factorization machine (FM) as a surrogate model for BBO. The FM model can be directly transformed into a quadratic unconstrained binary optimization model that can be solved using IMs. This makes it possible to optimize the acquisition function in BBO, which is a difficult task using conventional methods without IMs. Consequently, it has the advantage of handling large BBO problems. To be able to perform BBO with the FMQA algorithm immediately, we introduce the FMQA algorithm along with Python packages to run it. In addition, we review examples of applications of the FMQA algorithm in various fields, including physics, chemistry, materials science, and social sciences. These successful examples include binary and integer optimization problems, as well as more general optimization problems involving graphs, networks, and strings, using a binary variational autoencoder. We believe that BBO using the FMQA algorithm will become a key technology in IMs including quantum annealers.", "published": "2025-07-24T00:37:23+00:00"}
{"id": "2507.17998v1", "title": "Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold", "authors": ["Jaeho Shin", "Hyeonjae Gil", "Junwoo Jang", "Maani Ghaffari", "Ayoung Kim"], "categories": ["cs.CV"], "abstract": "Affine Grassmannian has been favored for expressing proximity between lines and planes due to its theoretical exactness in measuring distances among features. Despite this advantage, the existing method can only measure the proximity without yielding the distance as an explicit function of rigid body transformation. Thus, an optimizable distance function on the manifold has remained underdeveloped, stifling its application in registration problems. This paper is the first to explicitly derive an optimizable cost function between two Grassmannian features with respect to rigid body transformation ($\\mathbf{R}$ and $\\mathbf{t}$). Specifically, we present a rigorous mathematical proof demonstrating that the bases of high-dimensional linear subspaces can serve as an explicit representation of the cost. Finally, we propose an optimizable cost function based on the transformed bases that can be applied to the registration problem of any affine subspace. Compared to vector parameter-based approaches, our method is able to find a globally optimal solution by directly minimizing the geodesic distance which is agnostic to representation ambiguity. The resulting cost function and its extension to the inlier-set maximizing \\ac{BnB} solver have been demonstrated to improve the convergence of existing solutions or outperform them in various computer vision tasks. The code is available on https://github.com/joomeok/GrassmannRegistration.", "published": "2025-07-24T00:28:01+00:00"}
{"id": "2507.17980v1", "title": "Machine Learning Workflow for Analysis of High-Dimensional Order Parameter Space: A Case Study of Polymer Crystallization from Molecular Dynamics Simulations", "authors": ["Elyar Tourani", "Brian J. Edwards", "Bamin Khomami"], "categories": ["physics.comp-ph", "cs.LG"], "abstract": "Currently, identification of crystallization pathways in polymers is being carried out using molecular simulation-based data on a preset cut-off point on a single order parameter (OP) to define nucleated or crystallized regions. Aside from sensitivity to cut-off, each of these OPs introduces its own systematic biases. In this study, an integrated machine learning workflow is presented to accurately quantify crystallinity in polymeric systems using atomistic molecular dynamics data. Each atom is represented by a high-dimensional feature vector that combines geometric, thermodynamic-like, and symmetry-based descriptors. Low dimensional embeddings are employed to expose latent structural fingerprints within atomic environments. Subsequently, unsupervised clustering on the embeddings identified crystalline and amorphous atoms with high fidelity. After generating high quality labels with multidimensional data, we use supervised learning techniques to identify a minimal set of order parameters that can fully capture this label. Various tests were conducted to reduce the feature set, demonstrating that using only three order parameters is sufficient to recreate the crystallization labels. Based on these observed OPs, the crystallinity index (C-index) is defined as the logistic regression model's probability of crystallinity, remaining bimodal throughout the process and achieving over 0.98 classification performance (AUC). Notably, a model trained on one or a few snapshots enables efficient on-the-fly computation of crystallinity. Lastly, we demonstrate how the optimal C-index fit evolves during various stages of crystallization, supporting the hypothesis that entropy dominates early nucleation, while symmetry gains relevance later. This workflow provides a data-driven strategy for OP selection and a metric to monitor structural transformations in large-scale polymer simulations.", "published": "2025-07-23T23:02:10+00:00"}
{"id": "2507.17977v1", "title": "Improving the Computational Efficiency and Explainability of GeoAggregator", "authors": ["Rui Deng", "Ziqi Li", "Mingshu Wang"], "categories": ["cs.LG", "cs.AI"], "abstract": "Accurate modeling and explaining geospatial tabular data (GTD) are critical for understanding geospatial phenomena and their underlying processes. Recent work has proposed a novel transformer-based deep learning model named GeoAggregator (GA) for this purpose, and has demonstrated that it outperforms other statistical and machine learning approaches. In this short paper, we further improve GA by 1) developing an optimized pipeline that accelerates the dataloading process and streamlines the forward pass of GA to achieve better computational efficiency; and 2) incorporating a model ensembling strategy and a post-hoc model explanation function based on the GeoShapley framework to enhance model explainability. We validate the functionality and efficiency of the proposed strategies by applying the improved GA model to synthetic datasets. Experimental results show that our implementation improves the prediction accuracy and inference speed of GA compared to the original implementation. Moreover, explanation experiments indicate that GA can effectively captures the inherent spatial effects in the designed synthetic dataset. The complete pipeline has been made publicly available for community use (https://github.com/ruid7181/GA-sklearn).", "published": "2025-07-23T22:51:09+00:00"}
{"id": "2507.17958v1", "title": "VIBE: Video-Input Brain Encoder for fMRI Response Modeling", "authors": ["Daniel Carlstrom Schad", "Shrey Dixit", "Janis Keck", "Viktor Studenyak", "Aleksandr Shpilevoi", "Andrej Bicanski"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "We present VIBE, a two-stage Transformer that fuses multi-modal video, audio, and text features to predict fMRI activity. Representations from open-source models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a modality-fusion transformer and temporally decoded by a prediction transformer with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson correlations of 32.25 on in-distribution Friends S07 and 21.25 on six out-of-distribution films. An earlier iteration of the same architecture obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second overall in the Algonauts 2025 Challenge.", "published": "2025-07-23T22:02:56+00:00"}
{"id": "2507.17944v1", "title": "Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text", "authors": ["Hulayyil Alshammari", "Praveen Rao"], "categories": ["cs.CL", "cs.AI"], "abstract": "Large language models (LLMs) have rapidly transformed the creation of written materials. LLMs have led to questions about writing integrity, thereby driving the creation of artificial intelligence (AI) detection technologies. Adversarial attacks, such as standard and humanized paraphrasing, inhibit detectors' ability to detect machine-generated text. Previous studies have mainly focused on ChatGPT and other well-known LLMs and have shown varying accuracy across detectors. However, there is a clear gap in the literature about DeepSeek, a recently published LLM. Therefore, in this work, we investigate whether six generally accessible AI detection tools -- AI Text Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can consistently recognize text generated by DeepSeek. The detectors were exposed to the aforementioned adversarial attacks. We also considered DeepSeek as a detector by performing few-shot prompting and chain-of-thought reasoning (CoT) for classifying AI and human-written text. We collected 49 human-authored question-answer pairs from before the LLM era and generated matching responses using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied adversarial techniques such as paraphrasing and humanizing to add 196 more samples. These were used to challenge detector robustness and assess accuracy impact. While QuillBot and Copyleaks showed near-perfect performance on original and paraphrased DeepSeek text, others -- particularly AI Text Classifier and GPT-2 -- showed inconsistent results. The most effective attack was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and 52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best five-shot result misclassifying only one of 49 samples (AI recall 96%, human recall 100%).", "published": "2025-07-23T21:26:33+00:00"}
{"id": "2507.17942v1", "title": "Minimax Data Sanitization with Distortion Constraint and Adversarial Inference", "authors": ["Amirarsalan Moatazedian", "Yauhen Yakimenka", "R\u00e9mi A. Chou", "J\u00f6rg Kliewer"], "categories": ["cs.IT", "cs.AI", "math.IT"], "abstract": "We study a privacy-preserving data-sharing setting where a privatizer transforms private data into a sanitized version observed by an authorized reconstructor and two unauthorized adversaries, each with access to side information correlated with the private data.   The reconstructor is evaluated under a distortion function, while each adversary is evaluated using a separate loss function. The privatizer ensures the reconstructor distortion remains below a fixed threshold while maximizing the minimum loss across the two adversaries. This two-adversary setting models cases where individual users cannot reconstruct the data accurately, but their combined side information enables estimation within the distortion threshold. The privatizer maximizes individual loss while permitting accurate reconstruction only through collaboration. This echoes secret-sharing principles, but with lossy rather than perfect recovery. We frame this as a constrained data-driven minimax optimization problem and propose a data-driven training procedure that alternately updates the privatizer, reconstructor, and adversaries. We also analyze the Gaussian and binary cases as special scenarios where optimal solutions can be obtained. These theoretical optimal results are benchmarks for evaluating the proposed minimax training approach.", "published": "2025-07-23T21:22:35+00:00"}
{"id": "2507.17930v1", "title": "How Software Engineers Engage with AI: A Pragmatic Process Model and Decision Framework Grounded in Industry Observations", "authors": ["Vahid Garousi", "Zafar Jafarov"], "categories": ["cs.SE"], "abstract": "Artificial Intelligence (AI) has the potential to transform Software Engineering (SE) by enhancing productivity, efficiency, and decision support. Tools like GitHub Copilot and ChatGPT have given rise to \"vibe coding\"-an exploratory, prompt-driven development style. Yet, how software engineers engage with these tools in daily tasks, especially in deciding whether to trust, refine, or reject AI-generated outputs, remains underexplored. This paper presents two complementary contributions. First, a pragmatic process model capturing real-world AI-assisted SE activities, including prompt design, inspection, fallback, and refinement. Second, a 2D decision framework that could help developers reason about trade-offs between effort saved and output quality. Grounded in practitioner reports and direct observations in three industry settings across Turkiye and Azerbaijan, our work illustrates how engineers navigate AI use with human oversight. These models offer structured, lightweight guidance to support more deliberate and effective use of AI tools in SE, contributing to ongoing discussions on practical human-AI collaboration.", "published": "2025-07-23T21:00:21+00:00"}
{"id": "2507.17924v1", "title": "UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction", "authors": ["Hongrong Yang", "Markus Schlaepfer"], "categories": ["cs.LG", "cs.AI"], "abstract": "Accurate population flow prediction is essential for urban planning, transportation management, and public health. Yet existing methods face key limitations: traditional models rely on static spatial assumptions, deep learning models struggle with cross-city generalization, and Large Language Models (LLMs) incur high computational costs while failing to capture spatial structure. Moreover, many approaches sacrifice resolution by clustering Points of Interest (POIs) or restricting coverage to subregions, limiting their utility for city-wide analytics. We introduce UrbanPulse, a scalable deep learning framework that delivers ultra-fine-grained, city-wide OD flow predictions by treating each POI as an individual node. It combines a temporal graph convolutional encoder with a transformer-based decoder to model multi-scale spatiotemporal dependencies. To ensure robust generalization across urban contexts, UrbanPulse employs a three-stage transfer learning strategy: pretraining on large-scale urban graphs, cold-start adaptation, and reinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS records from three metropolitan areas in California, UrbanPulse achieves state-of-the-art accuracy and scalability. Through efficient transfer learning, UrbanPulse takes a key step toward making high-resolution, AI-powered urban forecasting deployable in practice across diverse cities.", "published": "2025-07-23T20:44:25+00:00"}
{"id": "2507.17892v1", "title": "DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration", "authors": ["Hanzhou Liu", "Binghan Li", "Chengkai Liu", "Mi Lu"], "categories": ["cs.CV"], "abstract": "Transformers, with their self-attention mechanisms for modeling long-range dependencies, have become a dominant paradigm in image restoration tasks. However, the high computational cost of self-attention limits scalability to high-resolution images, making efficiency-quality trade-offs a key research focus. To address this, Restormer employs channel-wise self-attention, which computes attention across channels instead of spatial dimensions. While effective, this approach may overlook localized artifacts that are crucial for high-quality image restoration. To bridge this gap, we explore Dilated Neighborhood Attention (DiNA) as a promising alternative, inspired by its success in high-level vision tasks. DiNA balances global context and local precision by integrating sliding-window attention with mixed dilation factors, effectively expanding the receptive field without excessive overhead. However, our preliminary experiments indicate that directly applying this global-local design to the classic deblurring task hinders accurate visual restoration, primarily due to the constrained global context understanding within local attention. To address this, we introduce a channel-aware module that complements local attention, effectively integrating global context without sacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based architecture specifically designed for image restoration, achieves competitive results across multiple benchmarks, offering a high-quality solution for diverse low-level computer vision problems.", "published": "2025-07-23T19:41:49+00:00"}
{"id": "2507.17887v1", "title": "Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments", "authors": ["Wonjae Lee", "Taeyoung Kim", "Hyungbin Park"], "categories": ["cs.LG", "cs.NA", "math.NA"], "abstract": "This paper introduces an operator-based neural network, the mirror-padded Fourier neural operator (MFNO), designed to learn the dynamics of stochastic systems. MFNO extends the standard Fourier neural operator (FNO) by incorporating mirror padding, enabling it to handle non-periodic inputs. We rigorously prove that MFNOs can approximate solutions of path-dependent stochastic differential equations and Lipschitz transformations of fractional Brownian motions to an arbitrary degree of accuracy. Our theoretical analysis builds on Wong--Zakai type theorems and various approximation techniques. Empirically, the MFNO exhibits strong resolution generalization--a property rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet. Furthermore, our model achieves performance that is comparable or superior to these baselines while offering significantly faster sample path generation than classical numerical schemes.", "published": "2025-07-23T19:30:34+00:00"}
{"id": "2507.17882v1", "title": "A Sequential Unsupervised Learning Approach for Large, Multicolor, Photometric Surveys", "authors": ["Bradley D. Hutchinson", "Catherine A. Pilachowski", "Christian I. Johnson"], "categories": ["astro-ph.IM", "astro-ph.SR"], "abstract": "Observational astronomy has undergone a significant transformation driven by large-scale surveys, such as the Panoramic Survey Telescope and Rapid Response System (Pan-STARRS) Survey, the Sloan Digital Sky Survey (SDSS), and the Gaia Mission. These programs yield large, complex datasets that pose significant challenges for conventional analysis methods, and as a result, many different machine learning techniques are being tested and deployed. We introduce a new approach to analyzing multiband photometry by using a long-short term memory autoencoder (LSTM-AE). This model views mean measurements of multicolor photometry as a sequence in wavelength in order to encode patterns present in the stars' spectral energy distributions (SEDs) into a two-dimensional latent space. We showcase this by using Pan-STARRS grizy mean magnitudes, and we use globular clusters, labels from SIMBAD, Gaia DR3 parallaxes, and PanSTARRS images to aid our analysis and understanding of the latent space. For 3,112,259 stars in an annulus around the North Galactic Cap, 99.51% have their full SED shape reconstructed--that is the absolute difference between the observed and the model predicted magnitude in every band--within five hundredths of a magnitude. We show that the model likely denoises photometric data, potentially improving the quality of measurements. Lastly, we show that the detection of rare stellar types can be performed by analyzing poorly reconstructed photometry.", "published": "2025-07-23T19:16:08+00:00"}
{"id": "2507.17874v1", "title": "I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis", "authors": ["SaiBarath Sundar", "Pranav Satheesan", "Udayaadithya Avadhanam"], "categories": ["cs.AI"], "abstract": "Recent advances in agentic systems for data analysis have emphasized automation of insight generation through multi-agent frameworks, and orchestration layers. While these systems effectively manage tasks like query translation, data transformation, and visualization, they often overlook the structured reasoning process underlying analytical thinking. Reasoning large language models (LLMs) used for multi-step problem solving are trained as general-purpose problem solvers. As a result, their reasoning or thinking steps do not adhere to fixed processes for specific tasks. Real-world data analysis requires a consistent cognitive workflow: interpreting vague goals, grounding them in contextual knowledge, constructing abstract plans, and adapting execution based on intermediate outcomes. We introduce I2I-STRADA (Information-to-Insight via Structured Reasoning Agent for Data Analysis), an agentic architecture designed to formalize this reasoning process. I2I-STRADA focuses on modeling how analysis unfolds via modular sub-tasks that reflect the cognitive steps of analytical reasoning. Evaluations on the DABstep and DABench benchmarks show that I2I-STRADA outperforms prior systems in planning coherence and insight alignment, highlighting the importance of structured cognitive workflows in agent design for data analysis.", "published": "2025-07-23T18:58:42+00:00"}
{"id": "2507.17871v1", "title": "Shallow quantum circuit for generating O(1)-entanged approximate state designs", "authors": ["Wonjun Lee", "Minki Hhan", "Gil Young Cho", "Hyukjoon Kwon"], "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el", "physics.comp-ph"], "abstract": "Random quantum states have various applications in quantum information science, including quantum cryptography, quantum simulation, and benchmarking quantum devices. In this work, we discover a new ensemble of quantum states that serve as an $\\epsilon$-approximate state $t$-design while possessing extremely low entanglement, magic, and coherence. We show that those resources such quantum states can reach their theoretical lower bounds, $\\Omega\\left(\\log (t/\\epsilon)\\right)$, which are also proven in this work. This implies that for fixed $t$ and $\\epsilon$, those resources do not scale with the system size, i.e., $O(1)$ with respect to the total number of qubits $n$ in the system. Moreover, we explicitly construct an ancilla-free shallow quantum circuit for generating such states. To this end, we develop an algorithm that transforms $k$-qubit approximate state designs into $n$-qubit ones through a sequence of multi-controlled gates, without increasing the support size. The depth of such a quantum circuit is $O\\left(t [\\log t]^3 \\log n \\log(1/\\epsilon)\\right)$, which is the most efficient among existing algorithms without ancilla qubits. A class of shallow quantum circuits proposed in our work offers reduced cost for classical simulation of random quantum states, leading to potential applications in various quantum information processing tasks. As a concrete example for demonstrating utility of our algorithm, we propose classical shadow tomography using an $O(1)$-entangled estimator, which can achieve shorter runtime compared to conventional schemes.", "published": "2025-07-23T18:56:19+00:00"}
{"id": "2507.17836v1", "title": "Collapsar Disk Outflows III: Detectable Neutrino and Gravitational Wave Signatures", "authors": ["Rodrigo Fern\u00e1ndez", "Silas Janke", "Coleman Dean", "Irene Tamborra"], "categories": ["astro-ph.HE", "astro-ph.SR", "gr-qc", "nucl-th"], "abstract": "We investigate the neutrino and gravitational wave (GW) signals from accretion disks formed during the failed collapse of a rotating massive star (a collapsar). Following black hole formation, a neutrino-cooled, shocked accretion disk forms, which displays non-spherical oscillations for a period of seconds before becoming advective and exploding the star. We compute the neutrino and GW signals (matter quadrupole) from collapsar disks using global axisymmetric, viscous hydrodynamic simulations. The neutrino signal with typical energies of O$(10)$ MeV is maximal during the neutrino-cooled (NDAF) phase that follows shock formation. This phase lasts for a few seconds and is easily detectable within O$(10-100)$ kpc by the IceCube Neutrino Telescope. Additional neutrino signatures from a precursor equatorial shock and by stochastic accretion plumes during the advective phase are detectable within the galaxy. The GW signal during the NDAF phase is detectable in the galaxy by current and next-generation ground-based observatories. The explosion (memory) GW signal is similar to that of standard core-collapse supernovae and can be probed with a deci-Hertz space-based detector. Shock oscillations during the NDAF phase impart time variations with frequency O$(10-100)$ Hz to the neutrino and GW signals, encoding information about the shock dynamics and inner disk. These time variations can be detectable in neutrinos by IceCube within O$(1-10)$ kpc depending on progenitor model, flavor transformation scenario, and detailed properties of the angular momentum transport mechanism.", "published": "2025-07-23T18:02:40+00:00"}
{"id": "2507.17807v1", "title": "Deep learning approaches to top FCNC couplings to photons at the LHC", "authors": ["Benjamin Fuks", "Sumit K. Garg", "A. Hammad", "Adil Jueid"], "categories": ["hep-ph", "hep-ex"], "abstract": "We investigate the sensitivity of the LHC to flavour-changing neutral current interactions involving the top quark and a photon using a model-independent effective field theory framework, focusing on two complementary processes: single top production via $qg \\to t\\gamma$ and the rare decay $t \\to q\\gamma$ in top pair events. To enhance signal discrimination, we employ a range of deep learning classifiers, including multi-layer perceptrons, graph attention networks and transformers, and compare them against a traditional cut-based analysis. Our results demonstrate that attention-based architectures, in particular transformer networks, significantly outperform other strategies, yielding up to a factor of five improvement in the expected exclusion limits. In particular, we show that at the high-luminosity LHC, rare top branching ratios can be probed down to values as low as $10^{-6}$. Our results thus highlight the significant potential of attention-based architectures for improving the sensitivity to new physics signatures in top quark processes at colliders.", "published": "2025-07-23T18:00:01+00:00"}
{"id": "2507.17747v1", "title": "Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks", "authors": ["Linbo Cao", "Jinman Zhao"], "categories": ["cs.CL", "cs.AI"], "abstract": "As frontier language models increasingly saturate standard QA benchmarks, concerns about data contamination, memorization, and escalating dataset creation costs persist. We propose a debate-driven evaluation paradigm that transforms any existing QA dataset into structured adversarial debates--where one model is given the official answer to defend, and another constructs and defends an alternative answer--adjudicated by a judge model blind to the correct solution. By forcing multi-round argumentation, this approach substantially increases difficulty while penalizing shallow memorization, yet reuses QA items to reduce curation overhead. We make two main contributions: (1) an evaluation pipeline to systematically convert QA tasks into debate-based assessments, and (2) a public benchmark that demonstrates our paradigm's effectiveness on a subset of MMLU-Pro questions, complete with standardized protocols and reference models. Empirical results validate the robustness of the method and its effectiveness against data contamination--a Llama 3.1 model fine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%) but performed worse in debates. Results also show that even weaker judges can reliably differentiate stronger debaters, highlighting how debate-based evaluation can scale to future, more capable systems while maintaining a fraction of the cost of creating new benchmarks. Overall, our framework underscores that \"pretraining on the test set is no longer all you need,\" offering a sustainable path for measuring the genuine reasoning ability of advanced language models.", "published": "2025-07-23T17:58:14+00:00"}
{"id": "2507.17744v1", "title": "Yume: An Interactive World Generation Model", "authors": ["Xiaofeng Mao", "Shaoheng Lin", "Zhen Li", "Chuanhao Li", "Wenshuo Peng", "Tong He", "Jiangmiao Pang", "Mingmin Chi", "Yu Qiao", "Kaipeng Zhang"], "categories": ["cs.CV", "cs.AI", "cs.HC"], "abstract": "Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \\method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \\sekai to train \\method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/.", "published": "2025-07-23T17:57:09+00:00"}
{"id": "2507.17731v1", "title": "Flow Matching Meets Biology and Life Science: A Survey", "authors": ["Zihao Li", "Zhichen Zeng", "Xiao Lin", "Feihao Fang", "Yanru Qu", "Zhe Xu", "Zhining Liu", "Xuying Ning", "Tianxin Wei", "Ge Liu", "Hanghang Tong", "Jingrui He"], "categories": ["cs.LG", "cs.AI"], "abstract": "Over the past decade, advances in generative modeling, such as generative adversarial networks, masked autoencoders, and diffusion models, have significantly transformed biological research and discovery, enabling breakthroughs in molecule design, protein generation, drug discovery, and beyond. At the same time, biological applications have served as valuable testbeds for evaluating the capabilities of generative models. Recently, flow matching has emerged as a powerful and efficient alternative to diffusion-based generative modeling, with growing interest in its application to problems in biology and life sciences. This paper presents the first comprehensive survey of recent developments in flow matching and its applications in biological domains. We begin by systematically reviewing the foundations and variants of flow matching, and then categorize its applications into three major areas: biological sequence modeling, molecule generation and design, and peptide and protein generation. For each, we provide an in-depth review of recent progress. We also summarize commonly used datasets and software tools, and conclude with a discussion of potential future directions. The corresponding curated resources are available at https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.", "published": "2025-07-23T17:44:29+00:00"}
{"id": "2507.17728v1", "title": "Megrez2 Technical Report", "authors": ["Boxun Li", "Yadong Li", "Zhiyuan Li", "Congyi Liu", "Weilin Liu", "Guowei Niu", "Zheyue Tan", "Haiyang Xu", "Zhuyu Yao", "Tao Yuan", "Dong Zhou", "Yueqing Zhuang", "Bo Zhao", "Guohao Dai", "Yu Wang"], "categories": ["cs.CL"], "abstract": "We present Megrez2, a novel lightweight and high-performance language model architecture optimized for device native deployment. Megrez2 introduces a novel cross-layer expert sharing mechanism, which significantly reduces total parameter count by reusing expert modules across adjacent transformer layers while maintaining most of the model's capacity. It also incorporates pre-gated routing, enabling memory-efficient expert loading and faster inference. As the first instantiation of the Megrez2 architecture, we introduce the Megrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and further enhanced through supervised fine-tuning and reinforcement learning with verifiable rewards. With only 3B activated and 7.5B stored parameters, Megrez2-Preview demonstrates competitive or superior performance compared to larger models on a wide range of tasks, including language understanding, instruction following, mathematical reasoning, and code generation. These results highlight the effectiveness of the Megrez2 architecture to achieve a balance between accuracy, efficiency, and deployability, making it a strong candidate for real-world, resource-constrained applications.", "published": "2025-07-23T17:43:07+00:00"}
{"id": "2507.17721v1", "title": "Inflation from a generalized exponential plateau: towards extra suppressed tensor-to-scalar ratios", "authors": ["Gerasimos Kouniatalis", "Emmanuel N. Saridakis"], "categories": ["astro-ph.CO", "gr-qc"], "abstract": "We investigate a standard minimally-coupled scalar-field inflationary scenario, which is based on a new potential, with suitably generalized plateau features, that leads to extra small tensor-to-scalar ratios. In particular, we consider a specific three-parameter potential, which has a flatter plateau and a steeper well compared to the Starobinsky potential in the Einstein frame. We study the inflationary realization and we show that it guarantees a prolonged period of slow-roll inflation and a successful exit. Additionally, the steeper minimum leads to significantly suppressed tensor perturbations, and thus to an extra-small tensor-to-scalar ratio $r$, and we show that we are able to obtain $r$ values less than $10^{-5}$. Moreover, we calculate the reheating temperature showing that in order to be in agreement with observations one of the potential parameters should remain within specific bounds. Finally, performing an inverse conformal transformation to the Jordan frame we show that the considered potential corresponds to higher-order corrections to Starobinsky potential in the Einstein frame, and these corrections are the reason for the improved behavior of the tensor-to-scalar ratio.", "published": "2025-07-23T17:31:43+00:00"}
{"id": "2507.17691v1", "title": "CASCADE: LLM-Powered JavaScript Deobfuscator at Google", "authors": ["Shan Jiang", "Pranoy Kovuri", "David Tao", "Zhixun Tan"], "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.LG", "cs.PL"], "abstract": "Software obfuscation, particularly prevalent in JavaScript, hinders code comprehension and analysis, posing significant challenges to software testing, static analysis, and malware detection. This paper introduces CASCADE, a novel hybrid approach that integrates the advanced coding capabilities of Gemini with the deterministic transformation capabilities of a compiler Intermediate Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to identify critical prelude functions, the foundational components underlying the most prevalent obfuscation techniques, and leveraging JSIR for subsequent code transformations, CASCADE effectively recovers semantic elements like original strings and API names, and reveals original program behaviors. This method overcomes limitations of existing static and dynamic deobfuscation techniques, eliminating hundreds to thousands of hardcoded rules while achieving reliability and flexibility. CASCADE is already deployed in Google's production environment, demonstrating substantial improvements in JavaScript deobfuscation efficiency and reducing reverse engineering efforts.", "published": "2025-07-23T16:57:32+00:00"}
{"id": "2507.17672v1", "title": "Coupling all-electron full-potential density functional theory with grid-based continuum embeddings", "authors": ["Jakob Filser", "Edan Bainglass", "Karsten Reuter", "Oliviero Andreussi"], "categories": ["physics.comp-ph"], "abstract": "Recent advances in continuum embedding models have enabled the incorporation of solvent and electrolyte effects into density functional theory (DFT) simulations of material surfaces, significantly benefiting electrochemistry, catalysis, and other applications. To extend the simulation of diverse systems and properties, the implementation of continuum embedding models into the Environ library adopts a modular programming paradigm, offering a flexible interface for communication with various DFT programs. The speed and scalability of the current implementation rely on a smooth definition of the key physical properties of the atomistic system, in particular of its electronic density. This has hindered the coupling of Environ with all-electron simulation packages, as the sharp electron density peaks near atomic nuclei are difficult to represent on regular grids. In this work, we introduce a novel smoothing scheme that transforms atom-centered electron densities into a regular grid representation while preserving the accuracy of electrostatic calculations. This approach enables a minimal and generic interface, facilitating seamless interoperability between Environ and all-electron DFT programs. We demonstrate this development through the coupling of Environ with the FHI-aims package and present benchmark simulations that validate the proposed method.", "published": "2025-07-23T16:33:23+00:00"}
{"id": "2507.17662v1", "title": "Mammo-Mamba: A Hybrid State-Space and Transformer Architecture with Sequential Mixture of Experts for Multi-View Mammography", "authors": ["Farnoush Bayatmakou", "Reza Taleei", "Nicole Simone", "Arash Mohammadi"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "abstract": "Breast cancer (BC) remains one of the leading causes of cancer-related mortality among women, despite recent advances in Computer-Aided Diagnosis (CAD) systems. Accurate and efficient interpretation of multi-view mammograms is essential for early detection, driving a surge of interest in Artificial Intelligence (AI)-powered CAD models. While state-of-the-art multi-view mammogram classification models are largely based on Transformer architectures, their computational complexity scales quadratically with the number of image patches, highlighting the need for more efficient alternatives. To address this challenge, we propose Mammo-Mamba, a novel framework that integrates Selective State-Space Models (SSMs), transformer-based attention, and expert-driven feature refinement into a unified architecture. Mammo-Mamba extends the MambaVision backbone by introducing the Sequential Mixture of Experts (SeqMoE) mechanism through its customized SecMamba block. The SecMamba is a modified MambaVision block that enhances representation learning in high-resolution mammographic images by enabling content-adaptive feature refinement. These blocks are integrated into the deeper stages of MambaVision, allowing the model to progressively adjust feature emphasis through dynamic expert gating, effectively mitigating the limitations of traditional Transformer models. Evaluated on the CBIS-DDSM benchmark dataset, Mammo-Mamba achieves superior classification performance across all key metrics while maintaining computational efficiency.", "published": "2025-07-23T16:29:46+00:00"}
{"id": "2507.17657v1", "title": "Attention (as Discrete-Time Markov) Chains", "authors": ["Yotam Erel", "Olaf D\u00fcnkel", "Rishabh Dabral", "Vladislav Golyanik", "Christian Theobalt", "Amit H. Bermano"], "categories": ["cs.CV"], "abstract": "We introduce a new interpretation of the attention matrix as a discrete-time Markov chain. Our interpretation sheds light on common operations involving attention scores such as selection, summation, and averaging in a unified framework. It further extends them by considering indirect attention, propagated through the Markov chain, as opposed to previous studies that only model immediate effects. Our main observation is that tokens corresponding to semantically similar regions form a set of metastable states, where the attention clusters, while noisy attention scores tend to disperse. Metastable states and their prevalence can be easily computed through simple matrix multiplication and eigenanalysis, respectively. Using these lightweight tools, we demonstrate state-of-the-art zero-shot segmentation. Lastly, we define TokenRank -- the steady state vector of the Markov chain, which measures global token importance. We demonstrate that using it brings improvements in unconditional image generation. We believe our framework offers a fresh view of how tokens are being attended in modern visual transformers.", "published": "2025-07-23T16:20:47+00:00"}
{"id": "2507.17650v1", "title": "XStacking: Explanation-Guided Stacked Ensemble Learning", "authors": ["Moncef Garouani", "Ayah Barhrhouj", "Olivier Teste"], "categories": ["cs.LG"], "abstract": "Ensemble Machine Learning (EML) techniques, especially stacking, have been shown to improve predictive performance by combining multiple base models. However, they are often criticized for their lack of interpretability. In this paper, we introduce XStacking, an effective and inherently explainable framework that addresses this limitation by integrating dynamic feature transformation with model-agnostic Shapley additive explanations. This enables stacked models to retain their predictive accuracy while becoming inherently explainable. We demonstrate the effectiveness of the framework on 29 datasets, achieving improvements in both the predictive effectiveness of the learning space and the interpretability of the resulting models. XStacking offers a practical and scalable solution for responsible ML.", "published": "2025-07-23T16:14:48+00:00"}
{"id": "2507.17635v1", "title": "Gauge Symmetries, Exact Symmetries and Conserved Charges in Minimal Massive Gravity", "authors": ["Kang Liu", "Xiao-Mei Kuang"], "categories": ["hep-th"], "abstract": "In this paper, we investigate a three-dimensional gravitational model known as Minimal Massive Gravity (MMG), which includes an auxiliary field, using the covariant phase space method. Our analysis reveals the presence of three gauge symmetries whose algebras close via field recombination and parameter classification within this framework. Upon incorporating these additional symmetries within a specific limit of parameters, we find that the Kosmann derivative should be replaced by a novel transformation compatible with Wald's approach, which establishes a new mechanism for generating exact symmetries and constructing their corresponding conserved charge in theories with auxiliary fields, extending beyond standard methods. However, this transformation does not yield closed algebras on the space of fundamental fields. We find that this corresponds to a Lorentz vector that characterizes the approximate completeness of translation symmetry. As a result, we obtain a gauge invariant charge at a certain limit of parameters, which emerges as a nontrivial combination of the diffeomorphism charge and integrable gauge charges.", "published": "2025-07-23T16:02:27+00:00"}
{"id": "2507.17617v1", "title": "Reusing Attention for One-stage Lane Topology Understanding", "authors": ["Yang Li", "Zongzheng Zhang", "Xuchong Qiu", "Xinrun Li", "Ziming Liu", "Leichen Wang", "Ruikai Li", "Zhenxin Zhu", "Huan-ang Gao", "Xiaojian Lin", "Zhiyong Cui", "Hang Zhao", "Hao Zhao"], "categories": ["cs.CV"], "abstract": "Understanding lane toplogy relationships accurately is critical for safe autonomous driving. However, existing two-stage methods suffer from inefficiencies due to error propagations and increased computational overheads. To address these challenges, we propose a one-stage architecture that simultaneously predicts traffic elements, lane centerlines and topology relationship, improving both the accuracy and inference speed of lane topology understanding for autonomous driving. Our key innovation lies in reusing intermediate attention resources within distinct transformer decoders. This approach effectively leverages the inherent relational knowledge within the element detection module to enable the modeling of topology relationships among traffic elements and lanes without requiring additional computationally expensive graph networks. Furthermore, we are the first to demonstrate that knowledge can be distilled from models that utilize standard definition (SD) maps to those operates without using SD maps, enabling superior performance even in the absence of SD maps. Extensive experiments on the OpenLane-V2 dataset show that our approach outperforms baseline methods in both accuracy and efficiency, achieving superior results in lane detection, traffic element identification, and topology reasoning. Our code is available at https://github.com/Yang-Li-2000/one-stage.git.", "published": "2025-07-23T15:48:16+00:00"}
{"id": "2507.17616v1", "title": "Vision Transformer attention alignment with human visual perception in aesthetic object evaluation", "authors": ["Miguel Carrasco", "C\u00e9sar Gonz\u00e1lez-Mart\u00edn", "Jos\u00e9 Aranda", "Luis Oliveros"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Visual attention mechanisms play a crucial role in human perception and aesthetic evaluation. Recent advances in Vision Transformers (ViTs) have demonstrated remarkable capabilities in computer vision tasks, yet their alignment with human visual attention patterns remains underexplored, particularly in aesthetic contexts. This study investigates the correlation between human visual attention and ViT attention mechanisms when evaluating handcrafted objects. We conducted an eye-tracking experiment with 30 participants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal objects comprising basketry bags and ginger jars. Using a Pupil Labs eye-tracker, we recorded gaze patterns and generated heat maps representing human visual attention. Simultaneously, we analyzed the same objects using a pre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting attention maps from each of the 12 attention heads. We compared human and ViT attention distributions using Kullback-Leibler divergence across varying Gaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal correlation at sigma=2.4 +-0.03, with attention head #12 showing the strongest alignment with human visual patterns. Significant differences were found between attention heads, with heads #7 and #9 demonstrating the greatest divergence from human attention (p< 0.05, Tukey HSD test). Results indicate that while ViTs exhibit more global attention patterns compared to human focal attention, certain attention heads can approximate human visual behavior, particularly for specific object features like buckles in basketry items. These findings suggest potential applications of ViT attention mechanisms in product design and aesthetic evaluation, while highlighting fundamental differences in attention strategies between human perception and current AI models.", "published": "2025-07-23T15:47:34+00:00"}
{"id": "2507.17603v1", "title": "Citation Recommendation using Deep Canonical Correlation Analysis", "authors": ["Conor McNamara", "Effirul Ramlan"], "categories": ["cs.IR", "cs.LG"], "abstract": "Recent advances in citation recommendation have improved accuracy by leveraging multi-view representation learning to integrate the various modalities present in scholarly documents. However, effectively combining multiple data views requires fusion techniques that can capture complementary information while preserving the unique characteristics of each modality. We propose a novel citation recommendation algorithm that improves upon linear Canonical Correlation Analysis (CCA) methods by applying Deep CCA (DCCA), a neural network extension capable of capturing complex, non-linear relationships between distributed textual and graph-based representations of scientific articles. Experiments on the large-scale DBLP (Digital Bibliography & Library Project) citation network dataset demonstrate that our approach outperforms state-of-the-art CCA-based methods, achieving relative improvements of over 11% in Mean Average Precision@10, 5% in Precision@10, and 7% in Recall@10. These gains reflect more relevant citation recommendations and enhanced ranking quality, suggesting that DCCA's non-linear transformations yield more expressive latent representations than CCA's linear projections.", "published": "2025-07-23T15:34:07+00:00"}
{"id": "2507.17597v1", "title": "Explainable AI for Collaborative Assessment of 2D/3D Registration Quality", "authors": ["Sue Min Cho", "Alexander Do", "Russell H. Taylor", "Mathias Unberath"], "categories": ["cs.HC", "cs.CV"], "abstract": "As surgery embraces digital transformation--integrating sophisticated imaging, advanced algorithms, and robotics to support and automate complex sub-tasks--human judgment of system correctness remains a vital safeguard for patient safety. This shift introduces new \"operator-type\" roles tasked with verifying complex algorithmic outputs, particularly at critical junctures of the procedure, such as the intermediary check before drilling or implant placement. A prime example is 2D/3D registration, a key enabler of image-based surgical navigation that aligns intraoperative 2D images with preoperative 3D data. Although registration algorithms have advanced significantly, they occasionally yield inaccurate results. Because even small misalignments can lead to revision surgery or irreversible surgical errors, there is a critical need for robust quality assurance. Current visualization-based strategies alone have been found insufficient to enable humans to reliably detect 2D/3D registration misalignments. In response, we propose the first artificial intelligence (AI) framework trained specifically for 2D/3D registration quality verification, augmented by explainability features that clarify the model's decision-making. Our explainable AI (XAI) approach aims to enhance informed decision-making for human operators by providing a second opinion together with a rationale behind it. Through algorithm-centric and human-centered evaluations, we systematically compare four conditions: AI-only, human-only, human-AI, and human-XAI. Our findings reveal that while explainability features modestly improve user trust and willingness to override AI errors, they do not exceed the standalone AI in aggregate performance. Nevertheless, future work extending both the algorithmic design and the human-XAI collaboration elements holds promise for more robust quality assurance of 2D/3D registration.", "published": "2025-07-23T15:28:57+00:00"}
{"id": "2507.17596v2", "title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving", "authors": ["Maciej K. Wozniak", "Lianhang Liu", "Yixi Cai", "Patric Jensfelt"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "abstract": "While end-to-end autonomous driving models show promising results, their practical deployment is often hindered by large model sizes, a reliance on expensive LiDAR sensors and computationally intensive BEV feature representations. This limits their scalability, especially for mass-market vehicles equipped only with cameras. To address these challenges, we propose PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving architecture operates using only camera data, without explicit BEV representation and forgoing the need for LiDAR. PRIX leverages a visual feature extractor coupled with a generative planning head to predict safe trajectories from raw pixel inputs directly. A core component of our architecture is the Context-aware Recalibration Transformer (CaRT), a novel module designed to effectively enhance multi-level visual features for more robust planning. We demonstrate through comprehensive experiments that PRIX achieves state-of-the-art performance on the NavSim and nuScenes benchmarks, matching the capabilities of larger, multimodal diffusion planners while being significantly more efficient in terms of inference speed and model size, making it a practical solution for real-world deployment. Our work is open-source and the code will be at https://maxiuw.github.io/prix.", "published": "2025-07-23T15:28:23+00:00"}
{"id": "2507.17587v1", "title": "A Joint Planning Model for Fixed and Mobile Electric Vehicle Charging Stations Considering Flexible Capacity Strategy", "authors": ["Zhe Yu", "Xue Hu", "Qin Wang"], "categories": ["eess.SY", "cs.SY"], "abstract": "The widespread adoption of electric vehicles (EVs) has significantly increased demand on both transportation and power systems, posing challenges to their stable operation. To support the growing need for EV charging, both fixed charging stations (FCSs) and mobile charging stations (MCSs) have been introduced, serving as key interfaces between the power grid and traffic network. Recognizing the importance of collaborative planning across these sectors, this paper presents a two-stage joint planning model for FCSs and MCSs, utilizing an improved alternating direction method of multipliers (ADMM) algorithm. The primary goal of the proposed model is to transform the potential negative impacts of large-scale EV integration into positive outcomes, thereby enhancing social welfare through collaboration among multiple stakeholders. In the first stage, we develop a framework for evaluating FCS locations, incorporating assessments of EV hosting capacity and voltage stability. The second stage introduces a joint planning model for FCSs and MCSs, aiming to minimize the overall social costs of the EV charging system while maintaining a reliable power supply. To solve the planning problem, we employ a combination of mixed-integer linear programming, queueing theory, and sequential quadratic programming. The improved ADMM algorithm couples the siting and sizing decisions consistently by introducing coupling constraints, and supports a distributed optimization framework that coordinates the interests of EV users, MCS operators, and distribution system operators. Additionally, a flexible capacity planning strategy that accounts for the multi-period development potential of EVCS is proposed to reduce both the complexity and the investment required for FCS construction. Finally, a case study with comparative experiments demonstrates the effectiveness of the proposed models and solution methods.", "published": "2025-07-23T15:22:04+00:00"}
{"id": "2507.17581v1", "title": "A convergent sum-of-squares hierarchy for compiled nonlocal games", "authors": ["David Cui", "Chirag Falor", "Anand Natarajan", "Tina Zhang"], "categories": ["quant-ph"], "abstract": "We continue the line of work initiated by Kalai et al. (STOC '23), studying \"compiled\" nonlocal games played between a classical verifier and a single quantum prover, with cryptography simulating the spatial separation between the players. The central open question in this area is to understand the soundness of this compiler against quantum strategies, and apart from results for specific games, all that is known is the recent \"qualitative\" result of Kulpe et al. (STOC '25) showing that the success probability of a quantum prover in the compiled game is bounded by the game's quantum commuting-operator value in the limit as the cryptographic security parameter goes to infinity. In this work, we make progress towards a quantitative understanding of quantum soundness for general games, by giving a concrete framework to bound the quantum value of compiled nonlocal games. Building on the result of Kulpe et al. together with the notion of \"nice\" sum-of-squares certificates, introduced by Natarajan and Zhang (FOCS '23) to bound the value of the compiled CHSH game, we extend the niceness framework and construct a hierarchy of semidefinite programs that searches exclusively over nice certificates. We show that this hierarchy converges to the optimal quantum value of the game. Additionally, we present a transformation to make any degree-1 sum-of-squares certificate nice. This approach provides a systematic method to reproduce all known bounds for special classes of games together with Kulpe et al.'s bound for general games from the same framework.", "published": "2025-07-23T15:16:38+00:00"}
{"id": "2507.17577v1", "title": "Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based Priors", "authors": ["Chen Ma", "Xinjie Xu", "Shuyu Cheng", "Qi Xuan"], "categories": ["cs.CV", "cs.CR", "cs.LG", "I.2.6; I.5.1; G.1.6"], "abstract": "One of the most practical and challenging types of black-box adversarial attacks is the hard-label attack, where only the top-1 predicted label is available. One effective approach is to search for the optimal ray direction from the benign image that minimizes the $\\ell_p$-norm distance to the adversarial region. The unique advantage of this approach is that it transforms the hard-label attack into a continuous optimization problem. The objective function value is the ray's radius, which can be obtained via binary search at a high query cost. Existing methods use a \"sign trick\" in gradient estimation to reduce the number of queries. In this paper, we theoretically analyze the quality of this gradient estimation and propose a novel prior-guided approach to improve ray search efficiency both theoretically and empirically. Specifically, we utilize the transfer-based priors from surrogate models, and our gradient estimators appropriately integrate them by approximating the projection of the true gradient onto the subspace spanned by these priors and random directions, in a query-efficient manner. We theoretically derive the expected cosine similarities between the obtained gradient estimators and the true gradient, and demonstrate the improvement achieved by incorporating priors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that our approach significantly outperforms 11 state-of-the-art methods in terms of query efficiency.", "published": "2025-07-23T15:11:25+00:00"}
{"id": "2507.17558v1", "title": "Celestial Holography meets dS/CFT", "authors": ["Hideo Furugori", "Naoki Ogawa", "Sotaro Sugishita", "Takahiro Waki"], "categories": ["hep-th", "gr-qc"], "abstract": "We provide a concrete link between celestial amplitudes and cosmological correlators. We first construct a map from quantum field theories (QFTs) in $(D+2)$-dimensional Euclidean space to theories on the $(D+1)$-dimensional sphere, through a Weyl rescaling and a Fourier transformation. An analytic continuation extends this map to a relation between QFTs in Minkowski spacetime $\\text{M}_{D+2}$ and in de Sitter spacetime $\\text{dS}_{D+1}$ with the Bunch-Davies vacuum. Combining this relation with celestial holography, we show that the extrapolated operators in de Sitter space can be represented by operators on the celestial sphere $S^{D}$. Our framework offers a systematic route to transfer computational techniques and physical insights between celestial holography and the dS/CFT correspondence.", "published": "2025-07-23T14:49:21+00:00"}
{"id": "2507.17533v1", "title": "Multi-modal Multi-task Pre-training for Improved Point Cloud Understanding", "authors": ["Liwen Liu", "Weidong Yang", "Lipeng Ma", "Ben Fei"], "categories": ["cs.CV"], "abstract": "Recent advances in multi-modal pre-training methods have shown promising effectiveness in learning 3D representations by aligning multi-modal features between 3D shapes and their corresponding 2D counterparts. However, existing multi-modal pre-training frameworks primarily rely on a single pre-training task to gather multi-modal data in 3D applications. This limitation prevents the models from obtaining the abundant information provided by other relevant tasks, which can hinder their performance in downstream tasks, particularly in complex and diverse domains. In order to tackle this issue, we propose MMPT, a Multi-modal Multi-task Pre-training framework designed to enhance point cloud understanding. Specifically, three pre-training tasks are devised: (i) Token-level reconstruction (TLR) aims to recover masked point tokens, endowing the model with representative learning abilities. (ii) Point-level reconstruction (PLR) is integrated to predict the masked point positions directly, and the reconstructed point cloud can be considered as a transformed point cloud used in the subsequent task. (iii) Multi-modal contrastive learning (MCL) combines feature correspondences within and across modalities, thus assembling a rich learning signal from both 3D point cloud and 2D image modalities in a self-supervised manner. Moreover, this framework operates without requiring any 3D annotations, making it scalable for use with large datasets. The trained encoder can be effectively transferred to various downstream tasks. To demonstrate its effectiveness, we evaluated its performance compared to state-of-the-art methods in various discriminant and generative applications under widely-used benchmarks.", "published": "2025-07-23T14:13:14+00:00"}
{"id": "2507.17795v1", "title": "LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level Mobile Traffic Prediction", "authors": ["Shiyuan Zhang", "Tong Li", "Zhu Xiao", "Hongyang Du", "Kaibin Huang"], "categories": ["cs.LG"], "abstract": "Service-level mobile traffic prediction for individual users is essential for network efficiency and quality of service enhancement. However, current prediction methods are limited in their adaptability across different urban environments and produce inaccurate results due to the high uncertainty in personal traffic patterns, the lack of detailed environmental context, and the complex dependencies among different network services. These challenges demand advanced modeling techniques that can capture dynamic traffic distributions and rich environmental features. Inspired by the recent success of diffusion models in distribution modeling and Large Language Models (LLMs) in contextual understanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model (LSDM). LSDM integrates the generative power of diffusion models with the adaptive learning capabilities of transformers, augmented by the ability to capture multimodal environmental information for modeling service-level patterns and dynamics. Extensive evaluations on real-world service-level datasets demonstrate that the model excels in traffic usage predictions, showing outstanding generalization and adaptability. After incorporating contextual information via LLM, the performance improves by at least 2.83% in terms of the coefficient of determination. Compared to models of a similar type, such as CSDI, the root mean squared error can be reduced by at least 8.29%. The code and dataset will be available at: https://github.com/SoftYuaneR/LSDM.", "published": "2025-07-23T14:01:16+00:00"}
{"id": "2507.17518v1", "title": "Enabling Cyber Security Education through Digital Twins and Generative AI", "authors": ["Vita Santa Barletta", "Vito Bavaro", "Miriana Calvano", "Antonio Curci", "Antonio Piccinno", "Davide Pio Posa"], "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.HC", "cs.SE"], "abstract": "Digital Twins (DTs) are gaining prominence in cybersecurity for their ability to replicate complex IT (Information Technology), OT (Operational Technology), and IoT (Internet of Things) infrastructures, allowing for real time monitoring, threat analysis, and system simulation. This study investigates how integrating DTs with penetration testing tools and Large Language Models (LLMs) can enhance cybersecurity education and operational readiness. By simulating realistic cyber environments, this approach offers a practical, interactive framework for exploring vulnerabilities and defensive strategies. At the core of this research is the Red Team Knife (RTK), a custom penetration testing toolkit aligned with the Cyber Kill Chain model. RTK is designed to guide learners through key phases of cyberattacks, including reconnaissance, exploitation, and response within a DT powered ecosystem. The incorporation of Large Language Models (LLMs) further enriches the experience by providing intelligent, real-time feedback, natural language threat explanations, and adaptive learning support during training exercises. This combined DT LLM framework is currently being piloted in academic settings to develop hands on skills in vulnerability assessment, threat detection, and security operations. Initial findings suggest that the integration significantly improves the effectiveness and relevance of cybersecurity training, bridging the gap between theoretical knowledge and real-world application. Ultimately, the research demonstrates how DTs and LLMs together can transform cybersecurity education to meet evolving industry demands.", "published": "2025-07-23T13:55:35+00:00"}
{"id": "2507.17510v1", "title": "On the dimension-free control of higher order truncated Riesz transforms by higher order Riesz transforms", "authors": ["Maciej Kucharski", "Mateusz Kwa\u015bnicki", "B\u0142a\u017cej Wr\u00f3bel"], "categories": ["math.CA", "math.FA", "42B25, 42B20, 42B15"], "abstract": "Fix a positive integer $k$. Let $R_k$ be a higher order Riesz transform of order $k$ on $\\mathbb{R}^d$ and let $R_k^t,$ $t>0,$ be the corresponding truncated Riesz transform. We study the relation between $\\|R_k f\\|_{L^p(\\mathbb{R}^d)}$ and $\\|R_k^t f\\|_{L^p(\\mathbb{R}^d)}$ for $p=1$, $p=\\infty,$ and $p=2.$ This is performed via analysis of the factorization operator $M_k^t$ defined by the relation $R_k^t=M_k^t R_k.$ The operator $M_k^t$ is a convolution operator associated with an $L^1$ radial kernel $b_{k,d}^t(x)=t^{-d}b_{k,d}(x/t),$ where $b_{k,d}(x):=b_{k,d}^1(x).$   We prove that $b_{k,d} \\ge 0$ only for $k=1,2.$ We also justify that for fixed $k\\ge 3$ it holds \\[ \\lim_{d\\to \\infty}\\|b_{k,d}\\|_{L^1(\\mathbb{R}^d)}=\\infty. \\] This is contrary to the cases $k=1,2$ where it is known that $\\|b_{k,d}\\|_{L^1(\\mathbb{R}^d)}=1$. Finally, we show that for any positive integer $k$ the Fourier transform of $b_{k,d}$ is bounded in absolute value by $1.$ This implies the contractive estimate \\[ \\|R_k^t f\\|_{L^2(\\mathbb{R}^d)}\\le \\|R_k f\\|_{L^2(\\mathbb{R}^d)} \\] and an analogous estimate for general singular integrals with smooth kernels for radial input functions $f.$", "published": "2025-07-23T13:48:42+00:00"}
{"id": "2507.17508v1", "title": "Illicit object detection in X-ray imaging using deep learning techniques: A comparative evaluation", "authors": ["Jorgen Cani", "Christos Diou", "Spyridon Evangelatos", "Vasileios Argyriou", "Panagiotis Radoglou-Grammatikis", "Panagiotis Sarigiannidis", "Iraklis Varlamis", "Georgios Th. Papadopoulos"], "categories": ["cs.CV"], "abstract": "Automated X-ray inspection is crucial for efficient and unobtrusive security screening in various public settings. However, challenges such as object occlusion, variations in the physical properties of items, diversity in X-ray scanning devices, and limited training data hinder accurate and reliable detection of illicit items. Despite the large body of research in the field, reported experimental evaluations are often incomplete, with frequently conflicting outcomes. To shed light on the research landscape and facilitate further research, a systematic, detailed, and thorough comparative evaluation of recent Deep Learning (DL)-based methods for X-ray object detection is conducted. For this, a comprehensive evaluation framework is developed, composed of: a) Six recent, large-scale, and widely used public datasets for X-ray illicit item detection (OPIXray, CLCXray, SIXray, EDS, HiXray, and PIDray), b) Ten different state-of-the-art object detection schemes covering all main categories in the literature, including generic Convolutional Neural Network (CNN), custom CNN, generic transformer, and hybrid CNN-transformer architectures, and c) Various detection (mAP50 and mAP50:95) and time/computational-complexity (inference time (ms), parameter size (M), and computational load (GFLOPS)) metrics. A thorough analysis of the results leads to critical observations and insights, emphasizing key aspects such as: a) Overall behavior of the object detection schemes, b) Object-level detection performance, c) Dataset-specific observations, and d) Time efficiency and computational complexity analysis. To support reproducibility of the reported experimental results, the evaluation code and model weights are made publicly available at https://github.com/jgenc/xray-comparative-evaluation.", "published": "2025-07-23T13:47:33+00:00"}
{"id": "2507.17501v1", "title": "DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD", "authors": ["Xianbiao Qi", "Marco Chen", "Wenjie Xiao", "Jiaquan Ye", "Yelin He", "Chun-Guang Li", "Zhouchen Lin"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "abstract": "Transformers have become the de facto backbone of modern deep learning, yet their training typically demands an advanced optimizer with adaptive learning rate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that it is mainly due to a heavy-tailed distribution of the gradients. In this paper, we introduce a Deeply Normalized Transformer (DNT), which is meticulously engineered to overcome this limitation enabling seamless training with vanilla mSGDW while yielding comparable performance to the Transformers trained via AdamW. To be specific, in DNT, we strategically integrate normalization techniques at proper positions in the Transformers to effectively modulate the Jacobian matrices of each layer, balance the influence of weights, activations, and their interactions, and thus enable the distributions of gradients concentrated. We provide both theoretical justifications of the normalization technique used in our DNT and extensive empirical evaluation on two popular Transformer architectures to validate that: a) DNT outperforms its counterparts (\\ie, ViT and GPT), and b) DNT can be effectively trained with vanilla mSGDW.", "published": "2025-07-23T13:37:23+00:00"}
{"id": "2507.17459v1", "title": "Decoupling the i.i.d. field and the randomisation field in the Curie-Weiss model", "authors": ["Yacine Barhoumi-Andr\u00e9ani", "Peter Eichelsbacher"], "categories": ["math.PR"], "abstract": "Using the De Finetti representation of the Curie-Weiss model, the uniform coupling of Bernoulli random variables and the Laplace inversion formula (almost surely), we show that the full phase diagram of the Curie-Weiss model can be explained by a competition between the De Finetti randomisation and an approximate Gaussian process indexed by a complex variable that is equal to the inverse Laplace transform on a complex line of a Brownian Bridge. A more refined process type of rescaling shows that this is a modification of the Brownian Sheet that is at the core of all Gaussian random variables in the limits obtained in the model. This almost sure Laplace inversion approach allows moreover to treat all types of spin laws in the same vein as the Curie-Weiss Bernoulli spins.   This gives a natural explanation of several results that already appeared in the literature in the subcritical and critical case in addition to produce new analogous results in the super-critical case.   The functional approach here defined can moreover be extended to a wide class of statistical mechanical models that includes the Ising model in any dimension.", "published": "2025-07-23T12:39:01+00:00"}
